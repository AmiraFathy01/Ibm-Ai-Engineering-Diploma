{"cells":[{"cell_type":"markdown","id":"fe1c6d2d-0bf8-4946-baf4-67db1fb6e512","metadata":{},"outputs":[],"source":["\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork/images/IDSN-logo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"\u003e\n","\n","\u003ch1 align=\"center\"\u003e\u003cfont size=\"5\"\u003eRESTRICTED BOLTZMANN MACHINES\u003c/font\u003e\u003c/h1\u003e\n"]},{"cell_type":"markdown","id":"d52b5ec0-3f53-47d2-8045-c736beb23245","metadata":{},"outputs":[],"source":["Estimated time needed: **25** minutes\n"]},{"cell_type":"markdown","id":"cdb605b0-b4ee-4cdb-a0f2-5afc262e93c8","metadata":{},"outputs":[],"source":["\u003ch3\u003eIntroduction\u003c/h3\u003e\n","\u003cb\u003eRestricted Boltzmann Machine (RBM):\u003c/b\u003e  RBMs are shallow neural nets that learn to reconstruct data by themselves in an unsupervised fashion.  \n","\n","\n","\u003ch4\u003eWhy are RBMs important?\u003c/h4\u003e\n","An RBM are a basic form of autoencoder.  It can automatically extract \u003cb\u003emeaningful\u003c/b\u003e features from a given input.\n","\n","\n","\u003ch4\u003eHow does it work?\u003c/h4\u003e\n","RBM is a 2 layer neural network. Simply, RBM takes the inputs and translates those into a set of binary values that represents them in the hidden layer. Then, these numbers can be translated back to reconstruct the inputs. Through several forward and backward passes, the RBM will be trained, and a trained RBM can reveal which features are the most important ones when detecting patterns.   \n","\n","\n","\u003ch4\u003eWhat are the applications of an RBM?\u003c/h4\u003e\n","RBM is useful for \u003ca href='http://www.cs.utoronto.ca/~hinton/absps/netflixICML.pdf?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork954-2023-01-01'\u003e  Collaborative Filtering\u003c/a\u003e, dimensionality reduction, classification, regression, feature learning, topic modeling and even \u003cb\u003eDeep Belief Networks\u003c/b\u003e.\n","\n","\u003ch4\u003eIs RBM a generative or Discriminative model?\u003c/h4\u003e\n","RBM is a generative model. Let me explain it by first, see what is different between discriminative and generative models: \n","\n","\u003cb\u003eDiscriminative:\u003c/b\u003e Consider a classification problem where we want to learn to distinguish between Sedan cars (y = 1) and SUV cars (y = 0), based on some features of cars. Given a training set, an algorithm like logistic regression tries to find a straight line, or \u003ci\u003edecision boundary\u003c/i\u003e, that separates the suv and sedan.  \n","\n","\u003cb\u003eGenerative:\u003c/b\u003e looking at cars, we can build a model of what Sedan cars look like. Then, looking at SUVs, we can build a separate model of what SUV cars look like. Finally, to classify a new car, we can match the new car against the Sedan model, and match it against the SUV model, to see whether the new car looks more like the SUV or Sedan. \n","\n","Generative Models specify a probability distribution over a dataset of input vectors. We can carry out both supervised and unsupervised tasks with generative models:\n","\u003cul\u003e\n","    \u003cli\u003eIn an unsupervised task, we try to form a model for $P(x)$, where $P$ is the probability given $x$ as an input vector.\u003c/li\u003e\n","    \u003cli\u003eIn the supervised task, we first form a model for $P(x|y)$, where $P$ is the probability of $x$ given $y$(the label for $x$). For example, if $y = 0$ indicates that a car is an SUV, and $y = 1$ indicates that a car is a sedan, then $p(x|y = 0)$ models the distribution of SUV features, and $p(x|y = 1)$ models the distribution of sedan features. If we manage to find $P(x|y)$ and $P(y)$, then we can use \u003cb\u003eBayes rule\u003c/b\u003e to estimate $P(y|x)$, because:   \n","        $$p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$$\u003c/li\u003e\n","\u003c/ul\u003e\n","Now the question is, can we build a generative model, and then use it to create synthetic data by directly sampling from the modeled probability distributions? Lets see. \n"]},{"cell_type":"markdown","id":"267871b6-1fb3-43bc-abb2-76f7de374490","metadata":{},"outputs":[],"source":["\u003ch2\u003eTable of Contents\u003c/h2\u003e\n","\u003col\u003e\n","    \u003cli\u003e\u003ca href=\"#ref1\"\u003eInitialization\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#ref2\"\u003eRBM layers\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#ref3\"\u003eWhat RBM can do after training?\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#ref4\"\u003eHow to train the model?\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#ref5\"\u003eLearned features\u003c/a\u003e\u003c/li\u003e\n","\u003c/ol\u003e\n","\u003cp\u003e\u003c/p\u003e\n","\u003c/div\u003e\n","\u003cbr\u003e\n","\n","\u003chr\u003e\n"]},{"cell_type":"markdown","id":"eeab3e3c-2267-4d9d-ae3e-7c678d0c2c89","metadata":{},"outputs":[],"source":["\u003ca id=\"ref1\"\u003e\u003c/a\u003e\n","\u003ch3\u003eInitialization\u003c/h3\u003e\n","\n","First, we have to load the utility file which contains different utility functions that are not connected\n","in any way to the networks presented in the tutorials, but rather help in\n","processing the outputs into a more understandable way.\n"]},{"cell_type":"code","id":"48765d73-89aa-41bf-bc31-9922da4f3df5","metadata":{},"outputs":[],"source":["import urllib.request\nwith urllib.request.urlopen(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork/labs/Week4/data/utils.py\") as url:\n    response = url.read()\ntarget = open('utils.py', 'w')\ntarget.write(response.decode('utf-8'))\ntarget.close()"]},{"cell_type":"markdown","id":"abc2e254-e8a3-4963-a703-d2d756b1c88e","metadata":{},"outputs":[],"source":["\u003ch2\u003eInstalling TensorFlow \u003c/h2\u003e\n","\n","We will installing TensorFlow version 2.9.0 and its required prerequistes.  Also installing pillow...\n"]},{"cell_type":"code","id":"7f319a08-6876-47ad-b15a-02c68f176114","metadata":{},"outputs":[],"source":["!pip install grpcio==1.24.3\n!pip install tensorflow==2.9.0\n!pip install pillow==8.1.0"]},{"cell_type":"markdown","id":"681a4e4f-ef2e-416a-a70c-946547b8aa12","metadata":{},"outputs":[],"source":["\u003cb\u003eNotice:\u003c/b\u003e This notebook has been created with TensorFlow version 2.9.0, and might not work with other versions. Therefore we check:\n"]},{"cell_type":"code","id":"b8801866-525d-4dc6-9970-624ac5f698e0","metadata":{},"outputs":[],"source":["import tensorflow as tf\nfrom IPython.display import Markdown, display\n\ndef printmd(string):\n    display(Markdown('# \u003cspan style=\"color:red\"\u003e'+string+'\u003c/span\u003e'))\n\n\nif not tf.__version__ == '2.9.0':\n    printmd('\u003c\u003c\u003c\u003c\u003c!!!!! ERROR !!!! please upgrade to TensorFlow 2.9.0, or restart your Kernel (Kernel-\u003eRestart \u0026 Clear Output)\u003e\u003e\u003e\u003e\u003e')"]},{"cell_type":"markdown","id":"e45094f2-4d4a-4fed-a2dd-4929a34fe5ea","metadata":{},"outputs":[],"source":["Now, we load in all the packages that we use to create the net including the TensorFlow package:\n"]},{"cell_type":"code","id":"50bdebfb-45a5-4ca1-a27b-0f0418cf8f5d","metadata":{},"outputs":[],"source":["import tensorflow as tf\nimport numpy as np\n\nfrom PIL import Image\nfrom utils import tile_raster_images\nimport matplotlib.pyplot as plt\n%matplotlib inline"]},{"cell_type":"markdown","id":"1221efcd-a883-473d-bb86-a34ee68c8ea9","metadata":{},"outputs":[],"source":["\u003chr\u003e\n"]},{"cell_type":"markdown","id":"77b0892d-d47e-4590-b2c5-dc8c0e3d7752","metadata":{},"outputs":[],"source":["\u003ca id=\"ref2\"\u003e\u003c/a\u003e\n","\u003ch3\u003eRBM layers\u003c/h3\u003e\n","\n","An RBM has two layers. The first layer of the RBM is called the \u003cb\u003evisible\u003c/b\u003e (or input layer). Imagine that our toy example, has only vectors with 7 values, so the visible layer must have $V=7$ input nodes. \n","The second layer is the \u003cb\u003ehidden\u003c/b\u003e layer, which has $H$ neurons in our case. Each hidden node takes on values of either 0 or 1 (i.e., $h_i = 1$ or $h_i$ = 0), with a probability that is a logistic function of the inputs it receives from the other $V$ visible units, called for example, $p(h_i = 1)$. For our toy sample, we'll use 2 nodes in the hidden layer, so $H = 2$.\n","\n","\u003ccenter\u003e\u003cimg src=\"https://ibm.box.com/shared/static/eu26opvcefgls6vnwuo29uwp0nudmokh.png\" alt=\"RBM Model\" style=\"width: 400px;\"\u003e\u003c/center\u003e\n"]},{"cell_type":"markdown","id":"349468b7-0ffd-4e8f-8d68-d9443be849a8","metadata":{},"outputs":[],"source":["     \n","\n","Each node in the first layer also has a \u003cb\u003ebias\u003c/b\u003e. We will denote the bias as $v_{bias}$, and this single value is shared among the $V$ visible units.\n","\n","The \u003cb\u003ebias\u003c/b\u003e of the second is defined similarly as $h_{bias}$, and this single value among the $H$ hidden units.\n"]},{"cell_type":"code","id":"2faea12d-bdae-421e-b61f-e9a70103e7bb","metadata":{},"outputs":[],"source":["v_bias = tf.Variable(tf.zeros([7]), tf.float32)\nh_bias = tf.Variable(tf.zeros([2]), tf.float32)"]},{"cell_type":"markdown","id":"48d08e21-a12d-4c41-a2f2-35c000783b8a","metadata":{},"outputs":[],"source":["We have to define weights among the input layer and hidden layer nodes. In the weight matrix, the number of rows are equal to the input nodes, and the number of columns are equal to the output nodes. We define a tensor $\\mathbf{W}$ of shape = (7,2), where the number of visible neurons = 7, and the number of hidden neurons = 2. \n"]},{"cell_type":"code","id":"e47a9ee0-0eb2-4184-8b46-5815eddc060b","metadata":{},"outputs":[],"source":["W = tf.constant(np.random.normal(loc=0.0, scale=1.0, size=(7, 2)).astype(np.float32))"]},{"cell_type":"markdown","id":"74b48a8e-be39-4b5a-a310-0be1f4c6381a","metadata":{},"outputs":[],"source":["\u003chr\u003e\n"]},{"cell_type":"markdown","id":"218458dc-8d87-4c24-b9aa-db6117cdc8f8","metadata":{},"outputs":[],"source":["\u003ca id=\"ref3\"\u003e\u003c/a\u003e\n","\u003ch3\u003eWhat RBM can do after training?\u003c/h3\u003e\n","Think of RBM as a model that has been trained based on images of a dataset of many SUV and sedan cars. Also, imagine that the RBM network has only two hidden nodes, where one node encodes the weight and, and the other encodes the size.  \n","In a sense, the different configurations represent different cars, where one is an SUV and the other is Sedan.  In a training process, through many forward and backward passes, the RBM adjust its weights to send a stronger signal to either the SUV node (0, 1) or the sedan node (1, 0) in the hidden layer, given the pixels of images. Now, given an SUV in hidden layer, which distribution of pixels should we expect? RBM can give you 2 things. First, it encodes your images in hidden layer. Second, it gives you the probability of observing a case, given some hidden values.\n","\n","\n","\u003ch3\u003eThe Inference Process\u003c/h3\u003e\n","\n","RBM has two phases:\n","\u003cul\u003e\n","    \u003cli\u003eForward Pass\u003c/li\u003e  \n","    \u003cli\u003eBackward Pass or Reconstruction\u003c/li\u003e\n","\u003c/ul\u003e\n","\n","\u003cb\u003ePhase 1) Forward pass:\u003c/b\u003e  \n","\n","Input one training sample (one image) $\\mathbf{x}$ through all visible nodes, and pass it to all hidden nodes. Processing happens in each node in the hidden layer. This computation begins by making stochastic decisions about whether to transmit that input or not (i.e. to determine the state of each hidden layer).  First, the probability vector is computed using the input feature vector $\\mathbf{x}$, the weight matrix $\\mathbf{W}$, and the bias term $h_{bias}$, as \n","\n","\n","$$p({h_j}|\\mathbf x)= \\sigma( \\sum_{i=1}^V W_{ij} x_i + h_{bias} )$$, \n","\n","where $\\sigma(z) = (1+e^{-z})^{-1}$ is the logistic function.\n","\n","\n","So, what does $p({h_j})$ represent? It is the \u003cb\u003eprobability distribution\u003c/b\u003e of the hidden units. That is, RBM uses inputs $x_i$ to make predictions about hidden node activations. For example, imagine that the hidden node activation values are [0.51 0.84] for the first training item. It tells you that the conditional probability for each hidden neuron for Phase 1 is: \n","\n","$$p(h_{1} = 1|\\mathbf{v}) = 0.51$$\n","$$p(h_{2} = 1|\\mathbf{v}) = 0.84$$\n","\n","As a result, for each row in the training set, vector of probabilities is generated.  In TensorFlow, this is referred to as a `tensor` with a shape of (1,2). \n","\n","We then turn unit $j$ with probability $p(h_{j}|\\mathbf{v})$, and turn it off with probability $1 - p(h_{j}|\\mathbf{v})$ by generating a uniform random number vector $\\mathbf{\\xi}$, and comparing it to the activation probability as \n","\n","\u003ccenter\u003eIf $\\xi_j\u003ep(h_{j}|\\mathbf{v})$, then $h_j=1$, else $h_j=0$.\u003c/center\u003e\n","\n","\n","Therefore, the conditional probability of a configuration of $\\mathbf{h}$ given $\\mathbf{v}$ (for a training sample) is:\n","\n","$$p(\\mathbf{h} \\mid \\mathbf{v}) = \\prod_{j=1}^H p(h_j \\mid \\mathbf{v})$$\n","\n","\n","where $H$ is the number of hidden units.\n","\n"]},{"cell_type":"markdown","id":"8d88de99-9eac-4b72-9ef4-d050160fcb2a","metadata":{},"outputs":[],"source":["Before we go further, let's look at a toy example for one case out of all input. Assume that we have a trained RBM, and a very simple input vector, such as [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0].   \n","Let's see what the output of forward pass would look like:\n"]},{"cell_type":"code","id":"64856f2f-4991-487c-9de4-bd090c9e8aa7","metadata":{},"outputs":[],"source":["X = tf.constant([[1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]], tf.float32)\n\nv_state = X\nprint (\"Input: \", v_state)\n\nh_bias = tf.constant([0.1, 0.1])\nprint (\"hb: \", h_bias)\nprint (\"w: \", W)\n\n# Calculate the probabilities of turning the hidden units on:\nh_prob = tf.nn.sigmoid(tf.matmul(v_state, W) + h_bias)  #probabilities of the hidden units\nprint (\"p(h|v): \", h_prob)\n\n# Draw samples from the distribution:\nh_state = tf.nn.relu(tf.sign(h_prob - tf.random.uniform(tf.shape(h_prob)))) #states\nprint (\"h0 states:\", h_state)"]},{"cell_type":"markdown","id":"5e5f1913-71e8-40ce-b266-5eb1f49ea46b","metadata":{},"outputs":[],"source":["\u003cb\u003ePhase 2) Backward Pass (Reconstruction):\u003c/b\u003e\n","The RBM reconstructs data by making several forward and backward passes between the visible and hidden layers.\n","\n","So, in the second phase (i.e. reconstruction phase), the samples from the hidden layer (i.e. $\\mathbf h$) becomes the input in the backward pass. The same weight matrix and visible layer biases are used to passed to the sigmoid function. The reproduced output is a reconstruction which is an approximation of the original input.\n"]},{"cell_type":"code","id":"35f26183-0036-4af6-b019-7c61780a2faa","metadata":{},"outputs":[],"source":["vb = tf.constant([0.1, 0.2, 0.1, 0.1, 0.1, 0.2, 0.1])\nprint (\"b: \", vb)\nv_prob = tf.nn.sigmoid(tf.matmul(h_state, tf.transpose(W)) + vb)\nprint (\"p(vi∣h): \", v_prob)\nv_state = tf.nn.relu(tf.sign(v_prob - tf.random.uniform(tf.shape(v_prob))))\nprint (\"v probability states: \", v_state)"]},{"cell_type":"markdown","id":"c0622071-a2aa-4f6e-bbe6-9a58d9ca792c","metadata":{},"outputs":[],"source":["RBM learns a probability distribution over the input, and then, after being trained, the RBM can generate new samples from the learned probability distribution. As you know, \u003cb\u003eprobability distribution\u003c/b\u003e, is a mathematical function that provides the probabilities of occurrence of different possible outcomes in an experiment.\n","\n","The (conditional) probability distribution over the visible units v is given by\n","\n","$$p(\\mathbf{v} \\mid \\mathbf{h}) = \\prod_{i=1}^V p(v_i \\mid \\mathbf{h}),$$\n","\n","\n","where,\n","\n","$$p(v_i \\mid \\mathbf{h}) = \\sigma\\left(\\sum_{j=1}^H W_{ji} h_j + v_{bias} \\right)$$\n","\n","so, given current state of hidden units and weights, what is the probability of generating [1. 0. 0. 1. 0. 0. 0.] in reconstruction phase, based on the above \u003cb\u003eprobability distribution\u003c/b\u003e function?\n"]},{"cell_type":"code","id":"992ccc3d-91de-40fd-9d28-1323bfb7b24c","metadata":{},"outputs":[],"source":["inp = X\nprint(\"input X:\" , inp.numpy())\n\nprint(\"probablity vector:\" , v_prob[0].numpy())\nv_probability = 1\n\nfor elm, p in zip(inp[0],v_prob[0]) :\n    if elm ==1:\n        v_probability *= p\n    else:\n        v_probability *= (1-p)\n\nprint(\"probability of generating X: \" , v_probability.numpy())"]},{"cell_type":"markdown","id":"04ff95c7-8dee-4837-ac2e-b2549675baec","metadata":{},"outputs":[],"source":["How similar are vectors $\\mathbf{x}$ and $\\mathbf{v}$? Of course, the reconstructed values most likely will not look anything like the input vector, because our network has not been trained yet. Our objective is to train the model in such a way that the input vector and reconstructed vector to be same. Therefore, based on how different the input values look to the ones that we just reconstructed, the weights are adjusted. \n"]},{"cell_type":"markdown","id":"c8c5b30a-e8b1-4f5c-a330-35afa2fd9bbc","metadata":{},"outputs":[],"source":["\u003chr\u003e\n"]},{"cell_type":"markdown","id":"6df2fdeb-258e-419a-bd8f-d355160dfa15","metadata":{},"outputs":[],"source":["\n","\u003ch2\u003eMNIST\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"3779926d-23c1-4d66-914f-7f078d9f6595","metadata":{},"outputs":[],"source":["We will be using the MNIST dataset to practice the usage of RBMs. The following cell loads the MNIST dataset.\n"]},{"cell_type":"code","id":"5d78f144-b7cb-4666-9804-9534cbf9cad8","metadata":{},"outputs":[],"source":["#loading training and test data\nmnist = tf.keras.datasets.mnist\n(trX, trY), (teX, teY) = mnist.load_data()\n\n# showing an example of the Flatten class and operation\nfrom tensorflow.keras.layers import Flatten\nflatten = Flatten(dtype='float32')\ntrX = flatten(trX/255.0)\ntrY = flatten(trY/255.0)"]},{"cell_type":"markdown","id":"75a2621a-f73c-4b61-8e8a-a22eb6a3af12","metadata":{},"outputs":[],"source":["Lets look at the dimension of the images.\n"]},{"cell_type":"markdown","id":"00b23490-c951-4b35-a900-2431a5095a70","metadata":{},"outputs":[],"source":["MNIST images have 784 pixels, so the visible layer must have 784 input nodes.  For our case, we'll use 50 nodes in the hidden layer, so i = 50.\n"]},{"cell_type":"code","id":"bb7cc0bc-5aa1-4676-bcfb-f6ae3dd1bab1","metadata":{},"outputs":[],"source":["vb = tf.Variable(tf.zeros([784]), tf.float32)\nhb = tf.Variable(tf.zeros([50]), tf.float32)"]},{"cell_type":"markdown","id":"92f25b3c-44a4-4342-9923-dddc9a2739e6","metadata":{},"outputs":[],"source":["Let $\\mathbf W$ be the Tensor of 784x50 (784 - number of visible neurons, 50 - number of hidden neurons) that represents weights between the neurons. \n"]},{"cell_type":"code","id":"cac85183-49e0-4225-abb3-54b0093a3f45","metadata":{},"outputs":[],"source":["W = tf.Variable(tf.zeros([784,50]), tf.float32)"]},{"cell_type":"markdown","id":"067c4e92-2a6c-4e0b-8b3f-19e7cad60c4d","metadata":{},"outputs":[],"source":["Lets define the visible layer:\n"]},{"cell_type":"code","id":"72f731e1-0533-4cd4-b5a2-d6b916e23d02","metadata":{},"outputs":[],"source":["v0_state = tf.Variable(tf.zeros([784]), tf.float32)\n\n#testing to see if the matrix product works\ntf.matmul( [v0_state], W)"]},{"cell_type":"markdown","id":"7a4347f8-e9f7-4bcb-9350-c9d132b84545","metadata":{},"outputs":[],"source":["Now, we can define hidden layer:\n"]},{"cell_type":"code","id":"abba6ab2-2950-4954-b08d-4a04defd8b93","metadata":{},"outputs":[],"source":["#computing the hidden nodes probability vector and checking shape\nh0_prob = tf.nn.sigmoid(tf.matmul([v0_state], W) + hb)  #probabilities of the hidden units\nprint(\"h0_state shape: \" , tf.shape(h0_prob))\n\n#defining a function to return only the generated hidden states \ndef hidden_layer(v0_state, W, hb):\n    h0_prob = tf.nn.sigmoid(tf.matmul([v0_state], W) + hb)  #probabilities of the hidden units\n    h0_state = tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob)))) #sample_h_given_X\n    return h0_state\n\n\nh0_state = hidden_layer(v0_state, W, hb)\nprint(\"first 15 hidden states: \", h0_state[0][0:15])"]},{"cell_type":"markdown","id":"76405c42-562c-4a41-ab0b-2d956368750c","metadata":{},"outputs":[],"source":["Now, we define reconstruction part:\n"]},{"cell_type":"code","id":"d069fab9-d9bd-4bf6-915f-9944cee81d88","metadata":{},"outputs":[],"source":["def reconstructed_output(h0_state, W, vb):\n    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb) \n    v1_state = tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob)))) #sample_v_given_h\n    return v1_state[0]\n\nv1_state = reconstructed_output(h0_state, W, vb)\nprint(\"hidden state shape: \", h0_state.shape)\nprint(\"v0 state shape:  \", v0_state.shape)\nprint(\"v1 state shape:  \", v1_state.shape)"]},{"cell_type":"markdown","id":"1ada2348-7da0-42bd-b392-08af72dda252","metadata":{},"outputs":[],"source":["\u003ch3\u003eWhat is the objective function?\u003c/h3\u003e\n","\n","\u003cb\u003eGoal\u003c/b\u003e: Maximize the likelihood of our data being drawn from that distribution\n","\n","\u003cb\u003eCalculate error:\u003c/b\u003e  \n","In each epoch, we compute the \"error\" as a sum of the squared difference between step 1 and step n,\n","e.g the error shows the difference between the data and its reconstruction.\n","\n","\u003cb\u003eNote:\u003c/b\u003e tf.reduce_mean computes the mean of elements across dimensions of a tensor.\n"]},{"cell_type":"code","id":"c04edd06-e384-4bd1-800c-8b202e954523","metadata":{},"outputs":[],"source":["def error(v0_state, v1_state):\n    return tf.reduce_mean(tf.square(v0_state - v1_state))\n\nerr = tf.reduce_mean(tf.square(v0_state - v1_state))\nprint(\"error\" , err.numpy())"]},{"cell_type":"markdown","id":"0a5650f7-3608-4e20-993d-404e38955b0a","metadata":{},"outputs":[],"source":["\u003ca id=\"ref4\"\u003e\u003c/a\u003e\n","\u003ch3\u003eTraining the Model\u003c/h3\u003e\n","\u003cb\u003eWarning...\u003c/b\u003e The following part is math-heavy, but you can skip it if you just want to run the cells in the next section.\n","\n","As mentioned, we want to give a high probability to the input data we train on. So, in order to train an RBM, we have to maximize the product of probabilities assigned to all rows $\\mathbf{v}$ (images) in the training set $\\mathbf{V}$ (a matrix, where each row of it is treated as a visible vector $\\mathbf{v}$)\n","\n","$$\\arg \\max_W \\prod_{\\mathbf{v}\\in\\mathbf{V}_T} p(\\mathbf{v})$$\n","\n","\n","which is equivalent to maximizing the expectation of the log probability, given as\n","\n","$$\\arg\\max_W\\left[ \\mathbb{E} \\left(\\prod_{\\mathbf v\\in \\mathbf V}\\text{log} \\left(p(\\mathbf v)\\right) \\right) \\right].$$\n","\n","\n","So, we have to update the weights $W_{ij}$  to increase $p(\\mathbf{v})$ for all $\\mathbf{v}$ in our training data during training. So we have to calculate the derivative:\n","\n","\n","$$\\frac{\\partial \\log p(\\mathbf v)}{\\partial W_{ij}}$$\n","\n","This cannot be easily done by typical \u003cb\u003egradient descent (SGD)\u003c/b\u003e, so we can use another approach, which has 2 steps:\n","\u003col\u003e\n","    \u003cli\u003eGibbs Sampling\u003c/li\u003e\n","    \u003cli\u003eContrastive Divergence\u003c/li\u003e\n","\u003c/ol\u003e    \n","    \n","\u003ch3\u003eGibbs Sampling\u003c/h3\u003e   \n","\n","\u003ch4\u003eGibbs Sampling Step 1\u003c/h4\u003e \n","Given an input vector $\\mathbf{v}$, we are using $p(\\mathbf{h}|\\mathbf{v})$ to predict the hidden values $\\mathbf{h}$. \n","  $$p({h_j}|\\mathbf v)= \\sigma\\left(\\sum_{i=1}^V W_{ij} v_i + h_{bias} \\right)$$\n","The samples are generated from this distribution by generating the uniform random variate vector $\\mathbf{\\xi} \\sim U[0,1]$ of length $H$ and comparing to the computed probabilities as\n","\n","\n","\u003ccenter\u003eIf $\\xi_j\u003ep(h_{j}|\\mathbf{v})$, then $h_j=1$, else $h_j=0$.\u003c/center\u003e\n","\n","\n","\u003ch4\u003eGibbs Sampling Step 2\u003c/h4\u003e \n","Then, knowing the hidden values, we use $p(\\mathbf v| \\mathbf h)$ for reconstructing of new input values v. \n","\n","   $$p({v_i}|\\mathbf h)= \\sigma\\left(\\sum_{j=1}^H W^{T}_{ij} h_j + v_{bias} \\right)$$\n","\n","\n","The samples are generated from this distribution by generating a uniform random variate vector $\\mathbf{\\xi} \\sim U[0,1]$ of length $V$ and comparing to the computed probabilities as\n","\n","\u003ccenter\u003eIf $\\xi_i\u003ep(v_{i}|\\mathbf{h})$, then $v_i=1$, else $v_i=0$.\u003c/center\u003e\n","\n","Let vectors $\\mathbf v_k$ and $\\mathbf h_k$ be for the $k$th iteration.  In general, the $kth$ state is generrated as: \n","\n","\n","    \n","\u003cb\u003eIteration\u003c/b\u003e $k$: \n","\n","$$\\mathbf v_{k-1} \\Rightarrow p(\\mathbf h_{k-1}|\\mathbf v_{k-1})\\Rightarrow \\mathbf h_{k-1}\\Rightarrow p(\\mathbf v_{k}|\\mathbf h_{k-1})\\Rightarrow \\mathbf v_k$$       \n","    \n","\u003ch3\u003eContrastive Divergence (CD-k)\u003c/h3\u003e\n","The update of the weight matrix is done during the Contrastive Divergence step. \n","\n","Vectors v0 and vk are used to calculate the activation probabilities for hidden values h0 and hk. The difference between the outer products of those probabilities with input vectors v0 and vk results in the update matrix:\n","\n","\n","$$\\Delta \\mathbf W_k =\\mathbf v_k \\otimes \\mathbf h_k - \\mathbf v_{k-1} \\otimes \\mathbf h_{k-1}$$\n","\n","Contrastive Divergence is actually matrix of values that is computed and used to adjust values of the $\\mathbf W$ matrix. Changing $\\mathbf W$ incrementally leads to training of the  $\\mathbf W$ values. Then, on each step (epoch), $\\mathbf W$ is updated using the following:\n","\n","$$\\mathbf W_k = \\mathbf W_{k-1} + \\alpha * \\Delta \\mathbf W_k$$\n","\n","\n","\n","Reconstruction steps:\n","\u003cul\u003e\n","    \u003cli\u003e Get one data point from data set, like \u003ci\u003ex\u003c/i\u003e, and pass it through the following steps:\u003c/li\u003e\n","    \n","\u003cb\u003eIteration\u003c/b\u003e $k=1$: \n","    \n","Sampling (starting with input image)\n","    $$\\mathbf x = \\mathbf v_0 \\Rightarrow p(\\mathbf h_0|\\mathbf v_0)\\Rightarrow \\mathbf h_0 \\Rightarrow p(\\mathbf v_1|\\mathbf h_0)\\Rightarrow \\mathbf v_1$$   \n","    followed by the CD-k step\n","$$\\Delta \\mathbf W_1 =\\mathbf v_1 \\otimes \\mathbf h_1 - \\mathbf v_{0} \\otimes \\mathbf h_{0}$$     \n","$$\\mathbf W_1 = \\mathbf W_{0} + \\alpha * \\Delta \\mathbf W_1$$ \n"," \n","\u003cli\u003e $\\mathbf v_1$ is the reconstruction of $\\mathbf x$ sent to the next iteration).\u003c/li\u003e\n","\n","\u003cb\u003eIteration\u003c/b\u003e $k=2$: \n","\n","Sampling (starting with $\\mathbf v_1$)\n","\n","$$\\mathbf v_1 \\Rightarrow p(\\mathbf h_1|\\mathbf v_1)\\Rightarrow \\mathbf h_1\\Rightarrow p(\\mathbf v_2|\\mathbf h_1)\\Rightarrow \\mathbf v_2$$   \n","\n","followed by the CD-k step\n","$$\\Delta \\mathbf W_2 =\\mathbf v_2 \\otimes \\mathbf h_2 - \\mathbf v_{1} \\otimes \\mathbf h_{1}$$     \n","$$\\mathbf W_2 = \\mathbf W_{1} + \\alpha * \\Delta \\mathbf W_2$$ \n","\n","\u003cli\u003e $\\mathbf v_2$ is the reconstruction of $\\mathbf v_1$ sent to the next iteration).\u003c/li\u003e    \n","      \n","\u003cb\u003eIteration\u003c/b\u003e $k=K$:\n","    \n","Sampling (starting with $\\mathbf v_{K-1}$)\n","\n","$$\\mathbf v_{K-1} \\Rightarrow p(\\mathbf h_{K-1}|\\mathbf v_{K-1})\\Rightarrow \\mathbf h_{K-1}\\Rightarrow p(\\mathbf v_K|\\mathbf h_{K-1})\\Rightarrow \\mathbf v_K$$   \n","\n","followed by the CD-k step\n","$$\\Delta \\mathbf W_K =\\mathbf v_K \\otimes \\mathbf h_K - \\mathbf v_{K-1} \\otimes \\mathbf h_{K-1}$$     \n","$$\\mathbf W_K = \\mathbf W_{K-1} + \\alpha * \\Delta \\mathbf W_K$$ \n","    \n","\u003cb\u003eWhat is $\\alpha$?\u003c/b\u003e  \n","Here, alpha is some small step size, and is also known as the \"learning rate\".\n"]},{"cell_type":"markdown","id":"2f60b14f-bf2d-45c4-8d0f-67b966c6fc01","metadata":{},"outputs":[],"source":["$K$ is adjustable, and good performance can be achieved with $K=1$, so that we just take one set of sampling steps per image.\n"]},{"cell_type":"code","id":"413c50ad-dcdb-422e-a95c-ba37840142d3","metadata":{},"outputs":[],"source":["h1_prob = tf.nn.sigmoid(tf.matmul([v1_state], W) + hb)\nh1_state = tf.nn.relu(tf.sign(h1_prob - tf.random.uniform(tf.shape(h1_prob)))) #sample_h_given_X"]},{"cell_type":"markdown","id":"f38794be-3ed5-4ec5-ac1f-b8ad51bc3333","metadata":{},"outputs":[],"source":["Lets look at the error of the first run:\n"]},{"cell_type":"code","id":"dc6cde30-4a2a-46b0-83aa-3c6043d3ff27","metadata":{},"outputs":[],"source":["print(\"error: \", error(v0_state, v1_state))"]},{"cell_type":"code","id":"4365ca66-201b-4020-a14a-554ef5fc0ba7","metadata":{},"outputs":[],"source":["#Parameters\nalpha = 0.01\nepochs = 1\nbatchsize = 200\nweights = []\nerrors = []\nbatch_number = 0\nK = 1\n\n#creating datasets\ntrain_ds = \\\n    tf.data.Dataset.from_tensor_slices((trX, trY)).batch(batchsize)\n\nfor epoch in range(epochs):\n    for batch_x, batch_y in train_ds:\n        batch_number += 1\n        for i_sample in range(batchsize):           \n            for k in range(K):\n                v0_state = batch_x[i_sample]\n                h0_state = hidden_layer(v0_state, W, hb)\n                v1_state = reconstructed_output(h0_state, W, vb)\n                h1_state = hidden_layer(v1_state, W, hb)\n\n                delta_W = tf.matmul(tf.transpose([v0_state]), h0_state) - tf.matmul(tf.transpose([v1_state]), h1_state)\n                W = W + alpha * delta_W\n\n                vb = vb + alpha * tf.reduce_mean(v0_state - v1_state, 0)\n                hb = hb + alpha * tf.reduce_mean(h0_state - h1_state, 0) \n\n                v0_state = v1_state\n\n            if i_sample == batchsize-1:\n                err = error(batch_x[i_sample], v1_state)\n                errors.append(err)\n                weights.append(W)\n                print ( 'Epoch: %d' % epoch, \n                       \"batch #: %i \" % batch_number, \"of %i\" % int(60e3/batchsize), \n                       \"sample #: %i\" % i_sample,\n                       'reconstruction error: %f' % err)\n\n"]},{"cell_type":"markdown","id":"bc4acc4f-7bd0-4972-b04e-0d62935c3912","metadata":{},"outputs":[],"source":["Let's take a look at the errors at the end of each batch:\n"]},{"cell_type":"code","id":"483655aa-c404-4825-8268-642de0e98ff7","metadata":{},"outputs":[],"source":["plt.plot(errors)\nplt.xlabel(\"Batch Number\")\nplt.ylabel(\"Error\")\nplt.show()"]},{"cell_type":"markdown","id":"34683bd4-aa3f-40ff-95f3-6d2e408059c4","metadata":{},"outputs":[],"source":["What is the final weight matrix $W$ after training?\n"]},{"cell_type":"code","id":"8378b12e-7580-498f-9742-9a7ec889d021","metadata":{},"outputs":[],"source":["print(W.numpy()) # a weight matrix of shape (50,784)"]},{"cell_type":"markdown","id":"00d2cf5f-b6f0-48df-b799-e029311d7e65","metadata":{},"outputs":[],"source":["\u003ca id=\"ref5\"\u003e\u003c/a\u003e\n","\u003ch3\u003eLearned features\u003c/h3\u003e \n"]},{"cell_type":"markdown","id":"fcf78092-01c7-414e-9d42-215995366fd8","metadata":{},"outputs":[],"source":["We can take each hidden unit and visualize the connections between that hidden unit and each element in the input vector. In our case, we have 50 hidden units. Lets visualize those.\n"]},{"cell_type":"markdown","id":"1fbde916-840a-4760-8a50-e89a73a71e14","metadata":{},"outputs":[],"source":["Let's plot the current weights:\n","\u003cb\u003etile_raster_images\u003c/b\u003e helps in generating an easy to grasp image from a set of samples or weights. It transforms the \u003cb\u003euw\u003c/b\u003e (with one flattened image per row of size 784), into an array (of size $28\\times28$) in which images are reshaped and laid out like tiles on a floor.\n"]},{"cell_type":"code","id":"6bc391b5-aa79-4ad2-9ea5-09490314b7c1","metadata":{},"outputs":[],"source":["tile_raster_images(X=W.numpy().T, img_shape=(28, 28), tile_shape=(5, 10), tile_spacing=(1, 1))\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n%matplotlib inline\nimage = Image.fromarray(tile_raster_images(X=W.numpy().T, img_shape=(28, 28) ,tile_shape=(5, 10), tile_spacing=(1, 1)))\n### Plot image\nplt.rcParams['figure.figsize'] = (18.0, 18.0)\nimgplot = plt.imshow(image)\nimgplot.set_cmap('gray')  "]},{"cell_type":"markdown","id":"05262757-095a-41e2-b8ae-dd609f30bd1a","metadata":{},"outputs":[],"source":["Each tile in the above visualization corresponds to a vector of connections between a hidden unit and visible layer's units. \n"]},{"cell_type":"markdown","id":"0caf7520-bc05-4402-a65a-9fd51381da6f","metadata":{},"outputs":[],"source":["Let's look at one of the learned weights corresponding to one of hidden units for example. In this particular square, the gray color represents weight = 0, and the whiter it is, the more positive the weights are (closer to 1). Conversely, the darker pixels are, the more negative the weights. The positive pixels will increase the probability of activation in hidden units (after multiplying by input/visible pixels), and negative pixels will decrease the probability of a unit hidden to be 1 (activated). So, why is this important?  So we can see that this specific square (hidden unit) can detect a feature (e.g. a \"/\" shape) and if it exists in the input.\n"]},{"cell_type":"code","id":"272c0395-4a26-4d3d-bff8-343e90710002","metadata":{},"outputs":[],"source":["from PIL import Image\nimage = Image.fromarray(tile_raster_images(X =W.numpy().T[10:11], img_shape=(28, 28),tile_shape=(1, 1), tile_spacing=(1, 1)))\n### Plot image\nplt.rcParams['figure.figsize'] = (4.0, 4.0)\nimgplot = plt.imshow(image)\nimgplot.set_cmap('gray')  "]},{"cell_type":"markdown","id":"9aed6c7f-8407-447f-8de2-7e9d77924135","metadata":{},"outputs":[],"source":["Let's look at the reconstruction of an image now. Imagine that we have a destructed image of figure 3. Lets see if our trained network can fix it:\n","\n","First we plot the image:\n"]},{"cell_type":"code","id":"e01dd16a-ba6d-427c-9556-c8f069a93038","metadata":{},"outputs":[],"source":["!wget -O destructed3.jpg  https://ibm.box.com/shared/static/vvm1b63uvuxq88vbw9znpwu5ol380mco.jpg\nimg = Image.open('destructed3.jpg')\nimg"]},{"cell_type":"markdown","id":"3395c842-a4f4-4323-80a4-0a94421905f6","metadata":{},"outputs":[],"source":["Now let's pass this image through the neural net:\n"]},{"cell_type":"code","id":"70d90bc5-a19c-43d1-bfa6-69a7200a497e","metadata":{},"outputs":[],"source":["# convert the image to a 1d numpy array\nsample_case = np.array(img.convert('I').resize((28,28))).ravel().reshape((1, -1))/255.0\n\nsample_case = tf.cast(sample_case, dtype=tf.float32)"]},{"cell_type":"markdown","id":"7d0602f1-4de6-4084-9d09-0004c93cb9de","metadata":{},"outputs":[],"source":["Feed the sample case into the network and reconstruct the output:\n"]},{"cell_type":"code","id":"a195f7c4-7d60-42d2-a2e9-c49fc5ae82b8","metadata":{},"outputs":[],"source":["hh0_p = tf.nn.sigmoid(tf.matmul(sample_case, W) + hb)\nhh0_s = tf.round(hh0_p)\n\nprint(\"Probability nodes in hidden layer:\" ,hh0_p)\nprint(\"activated nodes in hidden layer:\" ,hh0_s)\n\n# reconstruct\nvv1_p = tf.nn.sigmoid(tf.matmul(hh0_s, tf.transpose(W)) + vb)\n\nprint(vv1_p)\n#rec_prob = sess.run(vv1_p, feed_dict={ hh0_s: hh0_s_val, W: prv_w, vb: prv_vb})"]},{"cell_type":"markdown","id":"64898dd9-dfdb-49fe-b401-fd5af185ee8f","metadata":{},"outputs":[],"source":["Here we plot the reconstructed image:\n"]},{"cell_type":"code","id":"15f7f2da-4b1e-4a94-8e06-16151cd4d521","metadata":{},"outputs":[],"source":["img = Image.fromarray(tile_raster_images(X=vv1_p.numpy(), img_shape=(28, 28),tile_shape=(1, 1), tile_spacing=(1, 1)))\nplt.rcParams['figure.figsize'] = (4.0, 4.0)\nimgplot = plt.imshow(img)\nimgplot.set_cmap('gray') "]},{"cell_type":"markdown","id":"6baba190-9036-4b55-9167-9887860bb69a","metadata":{},"outputs":[],"source":["\u003chr\u003e\n","\n","## Want to learn more?\n","\n","Also, you can use __Watson Studio__ to run these notebooks faster with bigger datasets.__Watson Studio__ is IBM’s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, __Watson Studio__ enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of __Watson Studio__ users today with a free account at [Watson Studio](https://cocl.us/ML0120EN_DSX).This is the end of this lesson. Thank you for reading this notebook, and good luck on your studies.\n"]},{"cell_type":"markdown","id":"deec31a2-c2a8-4ae5-b359-cd2daa2f0954","metadata":{},"outputs":[],"source":["### Thanks for completing this lesson!\n","\n","Notebook created by: \u003ca href = \"https://ca.linkedin.com/in/saeedaghabozorgi\"\u003eSaeed Aghabozorgi\u003c/a\u003e\n","\n","Updated to TF 2.X by  \u003ca href=\"https://ca.linkedin.com/in/nilmeier?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork954-2023-01-01\"\u003e Jerome Nilmeier\u003c/a\u003e\u003cbr /\u003e\n"]},{"cell_type":"markdown","id":"583db76c-5616-4506-ae3d-b9c73f0c77f1","metadata":{},"outputs":[],"source":["### References:\n","https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine  \n","http://deeplearning.net/tutorial/rbm.html  \n","http://www.cs.utoronto.ca/~hinton/absps/netflixICML.pdf\u003cbr\u003e\n","http://imonad.com/rbm/restricted-boltzmann-machine/  \n"]},{"cell_type":"markdown","id":"5c1e7bfe-510b-491d-82b4-24005643d95c","metadata":{},"outputs":[],"source":["\u003chr\u003e\n","\n","Copyright \u0026copy; 2018 [Cognitive Class](https://cocl.us/DX0108EN_CC). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork954-2023-01-01).\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}