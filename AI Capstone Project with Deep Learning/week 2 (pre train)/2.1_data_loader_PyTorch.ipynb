{"cells":[{"cell_type":"markdown","id":"57696b4b-ef79-4705-bd9c-1fdbedbe8682","metadata":{},"outputs":[],"source":["\u003ca href=\"http://cocl.us/pytorch_link_top\"\u003e\n","    \u003cimg src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \"\u003e\n","\u003c/a\u003e \n"]},{"cell_type":"markdown","id":"66a652a7-bd21-4b14-911f-f39eb679c951","metadata":{},"outputs":[],"source":["\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"200\" alt=\"cognitiveclass.ai logo\"\u003e\n"]},{"cell_type":"markdown","id":"a1ee1c10-6862-474a-9f91-de55562860c8","metadata":{},"outputs":[],"source":["\u003ch1\u003eObjective\u003c/h1\u003e\u003cul\u003e\u003cli\u003e How to create a dataset object.\u003c/li\u003e\u003c/ul\u003e \n"]},{"cell_type":"markdown","id":"d8a41aae-847b-4d13-a561-de4a0001b83a","metadata":{},"outputs":[],"source":["\u003ch1\u003eData Preparation with PyTorch\u003c/h1\u003e\n"]},{"cell_type":"markdown","id":"6ecdb884-7598-46b6-acb5-e3911bde1738","metadata":{},"outputs":[],"source":["\u003cp\u003eCrack detection has vital importance for structural health monitoring and inspection. We would like to train a network to detect Cracks, we will denote the images that contain cracks as positive and images with no cracks as negative.  In this lab you are going to have to build a dataset object. There are five questions in this lab, Including some questions that are intermediate steps to help you build the dataset object. You are going to have to remember the output for some  of the questions. \u003c/p\u003e\n"]},{"cell_type":"markdown","id":"da45df08-3ba2-48af-9d69-9b8b6f16cb20","metadata":{},"outputs":[],"source":["\u003ch2\u003eTable of Contents\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"e83299f6-fab6-468c-88ed-d140911b571b","metadata":{},"outputs":[],"source":["\u003cdiv class=\"alert alert-block alert-info\" style=\"margin-top: 20px\"\u003e\n","\n","\n","\u003cul\u003e\n","    \u003cli\u003e\u003ca href=\"#auxiliary\"\u003e Imports and Auxiliary Functions \u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#download_data\"\u003e Download data\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#examine_files\"\u003eExamine Files\u003c/a\u003e\u003c/li\u003e \n","    \u003cli\u003e\u003ca href=\"#Question_1\"\u003e\u003cb\u003eQuestion 1:find number of files\u003c/b\u003e \u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#assign_labels\"\u003eAssign Labels to Images  \u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Question_2\"\u003e\u003cb\u003eQuestion 2 : Assign labels to image \u003c/b\u003e \u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#split\"\u003eTraining  and Validation  Split \u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Question_3\"\u003e\u003cb\u003eQuestion 3: Training  and Validation  Split\u003c/b\u003e \u003c/a\u003e\u003c/li\u003e\n","\u003cli\u003e\u003ca href=\"#data_class\"\u003eCreate a Dataset Class \u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Question_4\"\u003e\u003cb\u003eQuestion 4:Display  training dataset object\u003c/b\u003e \u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Question_5\"\u003e\u003cb\u003eQuestion 5:Display  validation dataset  object\u003c/b\u003e \u003c/a\u003e\u003c/li\u003e\n","\n","\u003c/ul\u003e\n","\u003cp\u003eEstimated Time Needed: \u003cstrong\u003e25 min\u003c/strong\u003e\u003c/p\u003e\n"," \u003c/div\u003e\n","\u003chr\u003e\n"]},{"cell_type":"markdown","id":"44ff5994-c272-4d20-97a2-1ef1c859e96d","metadata":{},"outputs":[],"source":["\u003ch2 id=\"auxiliary\"\u003eImports and Auxiliary Functions\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"572654b4-587e-4279-b13c-7977255d1b37","metadata":{},"outputs":[],"source":["The following are the libraries we are going to use for this lab:\n"]},{"cell_type":"code","id":"99b83815-414b-4a76-837d-6f07fa76274b","metadata":{},"outputs":[],"source":["from PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nimport torch\nfrom torch.utils.data import Dataset\nimport skillsnetwork \n"]},{"cell_type":"markdown","id":"1bd010de-f754-4850-89e9-7836e2f430ca","metadata":{},"outputs":[],"source":["We will use this function in the lab to plot:\n"]},{"cell_type":"code","id":"53c92cdc-02f7-4fac-8332-0e431670c46c","metadata":{},"outputs":[],"source":["def show_data(data_sample, shape = (28, 28)):\n    plt.imshow(data_sample[0].numpy().reshape(shape), cmap='gray')\n    plt.title('y = ' + data_sample[1])"]},{"cell_type":"markdown","id":"496f00b0-cfea-48ab-9b27-f401452ce1c8","metadata":{},"outputs":[],"source":["\u003ch2 id=\"download_data\"\u003eDownload Data\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"5e9e483c-2674-4911-832f-dd3c5798362a","metadata":{},"outputs":[],"source":["In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. \u003cb\u003eskillsnetwork.prepare\u003c/b\u003e is a command that's used to download a zip file, unzip it and store it in a specified directory. Locally we store the data in the directory  **/resources/data**. \n"]},{"cell_type":"code","id":"a098e000-7897-4d69-bdf6-a5fe7aff15e8","metadata":{},"outputs":[],"source":["await skillsnetwork.prepare(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\", path = \"/resources/data\", overwrite=True)"]},{"cell_type":"markdown","id":"91b92ba1-2f89-4541-885c-be5447f9f9bc","metadata":{},"outputs":[],"source":["We then download the files that contain the negative images:\n"]},{"cell_type":"markdown","id":"599b725f-2f11-4680-a3b0-aba669f9a39d","metadata":{},"outputs":[],"source":["\u003ch2 id=\"examine_files\"\u003eExamine Files \u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"060f7363-a02a-4869-9a00-e8e25736bbc1","metadata":{},"outputs":[],"source":["In the previous lab, we create two lists; one to hold the path to the Negative files and one to hold the path to the Positive files. This process is shown in the following few lines of code.\n"]},{"cell_type":"markdown","id":"c1d06ac4-bc21-4a55-b9e8-ab188d50dcf5","metadata":{},"outputs":[],"source":["We can obtain the list that contains the path to the \u003cb\u003enegative files\u003c/b\u003e as follows:\n"]},{"cell_type":"code","id":"1de91b62-1eab-41e3-addf-5ba364c257d6","metadata":{},"outputs":[],"source":["directory=\"/resources/data\"\nnegative='Negative'\nnegative_file_path=os.path.join(directory,negative)\nnegative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\nnegative_files.sort()\nnegative_files[0:3]"]},{"cell_type":"markdown","id":"990c1be7-2292-4ebf-b43d-ba2cb9ff90d7","metadata":{},"outputs":[],"source":["We can obtain the list that contains the path to the \u003cb\u003epositive files\u003c/b\u003e files as follows:\n"]},{"cell_type":"code","id":"d615b35a-54fd-4399-8dd8-00109401d4b3","metadata":{},"outputs":[],"source":["positive=\"Positive\"\npositive_file_path=os.path.join(directory,positive)\npositive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\npositive_files.sort()\npositive_files[0:3]"]},{"cell_type":"markdown","id":"50765b89-762e-415c-b4c0-897a8c9560fd","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Question_1\"\u003eQuestion 1\u003c/h2\u003e\n","\u003cb\u003eFind the \u003cb\u003ecombined\u003c/b\u003e length of the list \u003ccode\u003epositive_files\u003c/code\u003e and \u003ccode\u003enegative_files\u003c/code\u003e using the function \u003ccode\u003elen\u003c/code\u003e . Then assign  it to the variable \u003ccode\u003enumber_of_samples\u003c/code\u003e\u003c/b\u003e\n"]},{"cell_type":"code","id":"b7559bb3-5987-4779-87b8-11c08fcd4024","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"6b7330e3-1e19-4340-b5c4-fbc0504e7f9b","metadata":{},"outputs":[],"source":["\u003ch2 id=\"assign_labels\"\u003eAssign Labels to Images \u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"76d411ee-320a-44cc-8087-055e55e47c12","metadata":{},"outputs":[],"source":["In this section we will assign a label to each image in this case we  can assign the positive images, i.e images with a crack to a value one  and the negative images i.e images with out a crack to a value of zero \u003cb\u003eY\u003c/b\u003e. First we create a tensor or vector of zeros, each element corresponds to a new sample. The length of the tensor is equal to the number of samples.\n"]},{"cell_type":"code","id":"aec22e38-c541-4842-8df3-ca27d664801d","metadata":{},"outputs":[],"source":["Y=torch.zeros([number_of_samples])"]},{"cell_type":"markdown","id":"cef3c5d2-9c7c-4090-984c-486a0f523fc2","metadata":{},"outputs":[],"source":["As we are using the tensor \u003cb\u003eY\u003c/b\u003e for classification we cast it to a \u003ccode\u003eLongTensor\u003c/code\u003e. \n"]},{"cell_type":"code","id":"79dd865e-41fd-4c44-98dd-87b5f4f3f832","metadata":{},"outputs":[],"source":["Y=Y.type(torch.LongTensor)\nY.type()"]},{"cell_type":"markdown","id":"009f71f4-0414-49ca-9ab8-c29d5bf0e9c1","metadata":{},"outputs":[],"source":["With respect to each element we will set the even elements to class one and the odd elements to class zero.\n"]},{"cell_type":"code","id":"e99c8527-1005-4561-a70b-583ac43c93bd","metadata":{},"outputs":[],"source":["Y[::2]=1\nY[1::2]=0"]},{"cell_type":"markdown","id":"5a125d0e-b223-4344-acc5-b20a2d8c5d1e","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Question_2\"\u003eQuestion 2\u003c/h2\u003e\n","\u003cb\u003eCreate a list all_files such that the even indexes contain the path to images with positive or cracked samples and the odd element contain the negative images or images with out cracks. Then use the following code to print out the first four samples.\u003c/b\u003e\n"]},{"cell_type":"code","id":"0018422e-723c-4dfd-9d20-59930f8ff009","metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"3b2e1e10-6c7d-429e-b52a-bee8df1c35d2","metadata":{},"outputs":[],"source":["code used to print samples:\n"]},{"cell_type":"code","id":"352edbfe-9014-4e3a-a0b0-dd1e2637aae4","metadata":{},"outputs":[],"source":["for y,file in zip(Y, all_files[0:4]):\n    plt.imshow(Image.open(file))\n    plt.title(\"y=\"+str(y.item()))\n    plt.show()\n    "]},{"cell_type":"markdown","id":"27abbfa3-072c-42bc-a761-5cda964703fa","metadata":{},"outputs":[],"source":["\u003ch2 id=\"split\"\u003eTraining  and Validation  Split  \u003c/h2\u003e\n","When training the model we  split up our data into training and validation data. It If the variable train is set to \u003ccode\u003eTrue\u003c/code\u003e  the following lines of code will segment the  tensor \u003cb\u003eY\u003c/b\u003e such at  the first 30000 samples are used for training. If the variable train is set to \u003ccode\u003eFalse\u003c/code\u003e the remainder of the samples will be used for validation data. \n"]},{"cell_type":"code","id":"ab1c0364-6c2e-4270-bbb5-fd8a91e8dc9a","metadata":{},"outputs":[],"source":["train=False\n\nif train:\n    all_files=all_files[0:30000]\n    Y=Y[0:30000]\n\nelse:\n    all_files=all_files[30000:]\n    Y=Y[30000:]"]},{"cell_type":"markdown","id":"08dc6f5f-4ebf-4ebd-ae8f-c4d0b115fec0","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Question_3\"\u003eQuestion 3\u003c/h2\u003e\n","Modify the above lines of code such that if the variable \u003ccode\u003etrain\u003c/code\u003e is set to \u003cc\u003eTrue\u003c/c\u003e the first 30000 samples of all_files are use in training. If \u003ccode\u003etrain\u003c/code\u003e is set to \u003ccode\u003eFalse\u003c/code\u003e the remaining  samples are used for validation. In both cases reassign  the values to the variable all_files, then use the following lines of code to print out the first four validation sample images.\n"]},{"cell_type":"code","id":"9d6da5c0-0374-4ea2-b725-f31f46742734","metadata":{},"outputs":[],"source":["\n    "]},{"cell_type":"markdown","id":"d0a12722-f73c-405d-af48-ece20ad93c59","metadata":{},"outputs":[],"source":["Just a note the images printed out in question two are the first four training samples.\n"]},{"cell_type":"markdown","id":"456a850d-7b7a-4cb3-804b-8bd8fb4b61ef","metadata":{},"outputs":[],"source":["\u003ch2 id=\"data_class\"\u003eCreate a Dataset Class\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"e25ad484-a272-48c5-b39d-05eb69726393","metadata":{},"outputs":[],"source":["In this section, we will use the previous code to build a dataset class. \n"]},{"cell_type":"markdown","id":"4a120cff-e4b5-494f-a22e-cc1dc994b7ae","metadata":{},"outputs":[],"source":["\n","Complete the code to build a Dataset class \u003ccode\u003edataset\u003c/code\u003e. As before, make sure the even samples are positive, and the odd samples are negative.  If the parameter \u003ccode\u003etrain\u003c/code\u003e is set to \u003ccode\u003eTrue\u003c/code\u003e, use the first 30 000  samples as training data; otherwise, the remaining samples will be used as validation data.  \n"]},{"cell_type":"code","id":"08daec15-6d41-4365-99c2-4234beef1ae6","metadata":{},"outputs":[],"source":["class Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"/resources/data\"\n        positive=\"Positive\"\n        negative=\"Negative\"\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\n        positive_files.sort()\n        negative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\n        negative_files.sort()\n\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)\n    \n  \n            \n     \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n        \n        \n        image=Image.open(self.all_files[idx])\n        y=self.Y[idx]\n          \n        \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y"]},{"cell_type":"markdown","id":"cc7d343e-05a6-4eb1-b35a-7ba43716bd52","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Question_4\"\u003eQuestion 4\u003c/h2\u003e\n","\u003cb\u003eCreate a Dataset object \u003ccode\u003edataset\u003c/code\u003e for the training data, use the following lines of code to print out sample the 10th and  sample 100 (remember zero indexing)  \u003c/b\u003e\n"]},{"cell_type":"code","id":"ab1931be-3e55-4ff5-a0b4-a632461aaa4b","metadata":{},"outputs":[],"source":["\n\nfor sample  in samples:\n    plt.imshow(dataset[sample][0])\n    plt.xlabel(\"y=\"+str(dataset[sample][1].item()))\n    plt.title(\"training data, sample {}\".format(int(sample)))\n    plt.show()\n    "]},{"cell_type":"markdown","id":"094acb0b-2906-4084-9b23-f143a9778008","metadata":{},"outputs":[],"source":["We now have all the tools to create a list with the path to each image file.  We use a List Comprehensions  to make the code more compact. We assign it to the variable \u003ccode\u003enegative_files\u003ccode\u003e , sort it in and display the first three elements:\n"]},{"cell_type":"markdown","id":"dde4efea-19b8-4db1-bd06-3027781f3b1d","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Question_5\"\u003eQuestion 5\u003c/h2\u003e\n","\u003cb\u003eCreate a Dataset object \u003ccode\u003edataset\u003c/code\u003e for the validation  data, use the following lines of code to print out the 16 th and  sample 103 (remember zero indexing)   \u003c/b\u003e\n"]},{"cell_type":"code","id":"d8a1d019-03cf-4a72-9407-46f056c67e56","metadata":{},"outputs":[],"source":["\n\n\nfor sample  in samples:\n    plt.imshow(dataset[sample][0])\n    plt.xlabel(\"y=\"+str(dataset[sample][1].item()))\n    plt.title(\"validation data, sample {}\".format(int(sample)))\n    plt.show()"]},{"cell_type":"markdown","id":"caa49aa7-0d41-4f7a-a512-59fa3cb2be27","metadata":{},"outputs":[],"source":["\u003ch2\u003eAbout the Authors:\u003c/h2\u003e\n"," \u003ca href=\\\"https://www.linkedin.com/in/joseph-s-50398b136/\\\"\u003eJoseph Santarcangelo\u003c/a\u003e has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"bc9349a0-ccca-471f-adf6-16533fa8f600","metadata":{},"outputs":[],"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n"]},{"cell_type":"markdown","id":"914d8264-75a7-44d6-9e40-25c3be11567c","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"2fd091e0-4526-46e1-b14a-03411c293d69","metadata":{},"outputs":[],"source":["Copyright \u0026copy; 2018 \u003ca href=\"cognitiveclass.ai\"\u003ecognitiveclass.ai\u003c/a\u003e. This notebook and its source code are released under the terms of the \u003ca href=\\\"https://bigdatauniversity.com/mit-license/\\\"\u003eMIT License\u003c/a\u003e\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}