{"cells":[{"cell_type":"markdown","id":"087f7ec6-3412-4818-8c6c-129420355d4f","metadata":{},"outputs":[],"source":["\u003cp style=\"text-align:center\"\u003e\n","    \u003ca href=\"https://skills.network/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2023-01-01\"\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  /\u003e\n","    \u003c/a\u003e\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"65915d72-f585-403f-82cd-f89502d2d823","metadata":{},"outputs":[],"source":["\u003ch1\u003eMomentum\u003c/h1\u003e\n"]},{"cell_type":"markdown","id":"378548d2-0caf-4413-8ff5-605d0caa1f9e","metadata":{},"outputs":[],"source":["\n","\u003ch3\u003eObjective for this Notebook\u003ch3\u003e    \n","\u003ch5\u003e 1. Learn Saddle Points, Local Minima, and Noise\u003c/h5\u003e     \n","\n"]},{"cell_type":"markdown","id":"1b110906-7fa6-4744-9694-2a6f6f9e676c","metadata":{},"outputs":[],"source":["\u003ch2\u003eTable of Contents\u003c/h2\u003e\n","\u003cp\u003eIn this lab, you will deal with several problems associated with optimization and see how momentum can improve your results.\u003c/p\u003e\n","\u003cul\u003e\n","    \u003cli\u003e\u003ca href=\"#Saddle\"\u003eSaddle Points\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Minima\"\u003eLocal Minima\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Noise\"\u003e Noise \u003c/a\u003e\u003c/li\u003e\n","\u003c/ul\u003e\n","\n","\u003cp\u003eEstimated Time Needed: \u003cb\u003e25 min\u003c/b\u003e\u003c/p\u003e\n","\u003chr\u003e\n"]},{"cell_type":"markdown","id":"ee493f01-afbd-4e1c-b9db-f02693c538d3","metadata":{},"outputs":[],"source":["\u003ch2\u003ePreparation\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"509abc3a-c84e-4093-b70e-901a68002004","metadata":{},"outputs":[],"source":["Import the following libraries that you'll use for this lab:\n"]},{"cell_type":"code","id":"e8418fd8-37d6-45cb-933f-49aa7158f98d","metadata":{},"outputs":[],"source":["# These are the libraries that will be used for this lab.\n\nimport torch \nimport torch.nn as nn\nimport matplotlib.pylab as plt\nimport numpy as np\n\ntorch.manual_seed(0)"]},{"cell_type":"markdown","id":"94db2454-dbc4-4e88-992a-55f42926953e","metadata":{},"outputs":[],"source":["This function will plot a cubic function and the parameter values obtained via Gradient Descent.\n"]},{"cell_type":"code","id":"677edc9d-a4c1-4ae7-89e4-5029e44150f3","metadata":{},"outputs":[],"source":["# Plot the cubic\n\ndef plot_cubic(w, optimizer):\n    LOSS = []\n    # parameter values \n    W = torch.arange(-4, 4, 0.1)\n    # plot the loss fuction \n    for w.state_dict()['linear.weight'][0] in W:\n        LOSS.append(cubic(w(torch.tensor([[1.0]]))).item())\n    w.state_dict()['linear.weight'][0] = 4.0\n    n_epochs = 10\n    parameter = []\n    loss_list = []\n\n    # n_epochs\n    # Use PyTorch custom module to implement a ploynomial function\n    for n in range(n_epochs):\n        optimizer.zero_grad() \n        loss = cubic(w(torch.tensor([[1.0]])))\n        loss_list.append(loss)\n        parameter.append(w.state_dict()['linear.weight'][0].detach().data.item())\n        loss.backward()\n        optimizer.step()\n    plt.plot(parameter, [loss.detach().numpy().flatten()  for loss in loss_list], 'ro', label='parameter values')\n\n    plt.plot(W.numpy(), LOSS, label='objective function')\n    plt.xlabel('w')\n    plt.ylabel('l(w)')\n    plt.legend()"]},{"cell_type":"markdown","id":"4173e279-1caf-4752-84ba-8a30fc083c0c","metadata":{},"outputs":[],"source":["This function will plot a 4th order function and the parameter values obtained via Gradient Descent. You can also add Gaussian noise with a standard deviation determined by the parameter \u003ccode\u003estd\u003c/code\u003e.\n"]},{"cell_type":"code","id":"bf93ae61-861f-42ac-9c66-63bf68991a0a","metadata":{},"outputs":[],"source":["# Plot the fourth order function and the parameter values\n\ndef plot_fourth_order(w, optimizer, std=0, color='r', paramlabel='parameter values', objfun=True):\n    W = torch.arange(-4, 6, 0.1)\n    LOSS = []\n    for w.state_dict()['linear.weight'][0] in W:\n        LOSS.append(fourth_order(w(torch.tensor([[1.0]]))).item())\n    w.state_dict()['linear.weight'][0] = 6\n    n_epochs = 100\n    parameter = []\n    loss_list = []\n\n    #n_epochs\n    for n in range(n_epochs):\n        optimizer.zero_grad()\n        loss = fourth_order(w(torch.tensor([[1.0]]))) + std * torch.randn(1, 1)\n        loss_list.append(loss)\n        parameter.append(w.state_dict()['linear.weight'][0].detach().data.item())\n        loss.backward()\n        optimizer.step()\n    \n    # Plotting\n    if objfun:\n        plt.plot(W.numpy(), LOSS, label='objective function')\n    \n    plt.plot(parameter, [loss.detach().numpy().flatten()  for loss in loss_list], 'ro', label='paramlabel', color=color)\n    plt.xlabel('w')\n    plt.ylabel('l(w)')\n    plt.legend()"]},{"cell_type":"markdown","id":"eb645ae9-a5bd-418d-8251-78b160d62c93","metadata":{},"outputs":[],"source":["This is a custom module. It will behave like a single parameter value. We do it this way so we can use PyTorch's build-in optimizers .\n"]},{"cell_type":"code","id":"8416d9ca-a433-4ef1-a899-cbc5bc10db2b","metadata":{},"outputs":[],"source":["# Create a linear model\n\nclass one_param(nn.Module):\n    \n    # Constructor\n    def __init__(self, input_size, output_size):\n        super(one_param, self).__init__()\n        self.linear = nn.Linear(input_size, output_size, bias=False)\n        \n    # Prediction\n    def forward(self, x):\n        yhat = self.linear(x)\n        return yhat"]},{"cell_type":"markdown","id":"10e64430-4873-43f4-9b87-ae9e4a7d7a0b","metadata":{},"outputs":[],"source":["We create an object \u003ccode\u003ew\u003c/code\u003e, when we call the object with an input of one, it will behave like an individual parameter value. i.e \u003ccode\u003ew(1)\u003c/code\u003e is analogous to $w$ \n"]},{"cell_type":"code","id":"7a24c5b4-819d-4bc3-8cd8-a5a111f3b40a","metadata":{},"outputs":[],"source":["# Create a one_param object\n\nw = one_param(1, 1)"]},{"cell_type":"markdown","id":"c47922e0-347d-4ef5-b1b5-2e1dd5605db8","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"382d9519-5807-456b-8af0-abbc814aff85","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Saddle\"\u003eSaddle Points\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"5612acf9-38c1-4173-b7c5-08f92d3346e1","metadata":{},"outputs":[],"source":["Let's create a cubic function with Saddle points \n"]},{"cell_type":"code","id":"8aa46264-3820-48e5-b857-0292e37fb6e4","metadata":{},"outputs":[],"source":["# Define a function to output a cubic \n\ndef cubic(yhat):\n    out = yhat ** 3\n    return out"]},{"cell_type":"markdown","id":"9a36e5dd-0b18-408f-8abf-48c787cf8644","metadata":{},"outputs":[],"source":["We create an optimizer with no momentum term \n"]},{"cell_type":"code","id":"72da68ce-d2c5-4073-997a-eaea7fede7a3","metadata":{},"outputs":[],"source":["# Create a optimizer without momentum\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.01, momentum=0)"]},{"cell_type":"markdown","id":"78579cbd-f4fb-446a-9488-3fbaa2ce94c3","metadata":{},"outputs":[],"source":["We run several iterations of stochastic gradient descent and plot the results. We see the parameter values get stuck in the saddle point.\n"]},{"cell_type":"code","id":"9fd572d8-bdc3-4a91-a6da-cd6161f06fdf","metadata":{},"outputs":[],"source":["# Plot the model\n\nplot_cubic(w, optimizer)"]},{"cell_type":"markdown","id":"3fe59bd9-4a7d-447d-b252-07385d83704e","metadata":{},"outputs":[],"source":["we create an optimizer with momentum term of 0.9\n"]},{"cell_type":"code","id":"c903cedd-2a71-4349-9a89-611c21dd8a9f","metadata":{},"outputs":[],"source":["# Create a optimizer with momentum\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.01, momentum=0.9)"]},{"cell_type":"markdown","id":"df9fc594-e4b1-484f-937b-9242f2112e77","metadata":{},"outputs":[],"source":["We run several iterations of stochastic gradient descent with momentum and plot the results. We see the parameter values do not get stuck in the saddle point.\n"]},{"cell_type":"code","id":"aa911ad3-3d5b-4423-913c-4b05a5aedf96","metadata":{},"outputs":[],"source":["# Plot the model\n\nplot_cubic(w, optimizer)"]},{"cell_type":"markdown","id":"317a0f52-0e71-46e6-9c82-58559f2ca47d","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"53e4a781-c953-4187-a604-68e275828d43","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Minima\"\u003eLocal Minima\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"01c184d1-43ac-4a5a-baf9-8722abf0bd37","metadata":{},"outputs":[],"source":["In this section, we will create a fourth order polynomial with a local minimum at \u003ci\u003e4\u003c/i\u003e and a global minimum a \u003ci\u003e-2\u003c/i\u003e. We will then see how the momentum parameter affects convergence to a global minimum. The fourth order polynomial is given by:\n"]},{"cell_type":"code","id":"955e821a-5cc3-4e11-81aa-6ad37e4dbf60","metadata":{},"outputs":[],"source":["# Create a function to calculate the fourth order polynomial \n\ndef fourth_order(yhat): \n    out = torch.mean(2 * (yhat ** 4) - 9 * (yhat ** 3) - 21 * (yhat ** 2) + 88 * yhat + 48)\n    return out"]},{"cell_type":"markdown","id":"aebd7820-5316-4663-9684-eb360c42c6be","metadata":{},"outputs":[],"source":["We create an optimizer with no momentum term. We run several iterations of stochastic gradient descent and plot the results. We see the parameter values get stuck in the local minimum.\n"]},{"cell_type":"code","id":"7b9366c2-688a-43ce-ae0b-e17834eca9a2","metadata":{},"outputs":[],"source":["# Make the prediction without momentum\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.001)\nplot_fourth_order(w, optimizer)"]},{"cell_type":"markdown","id":"16d9b9a5-93a6-436b-b48c-4e2b533bcd3c","metadata":{},"outputs":[],"source":["We create an optimizer with a  momentum term of 0.9. We run several iterations of stochastic gradient descent and plot the results. We see the parameter values reach a global minimum.\n"]},{"cell_type":"code","id":"8fb1ddd3-8126-4ec2-a54c-982cc0533bc2","metadata":{},"outputs":[],"source":["# Make the prediction with momentum\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.001, momentum=0.9)\nplot_fourth_order(w, optimizer)"]},{"cell_type":"markdown","id":"e02fec91-a51e-4e70-8002-66d9d351def4","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"19f7c2fc-64d0-4f84-8d6a-3bbbda776479","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Noise\"\u003eNoise\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"9a290cce-5a48-4d14-9616-f6b0c3efde77","metadata":{},"outputs":[],"source":["In this section, we will create a fourth order polynomial with a local minimum at 4 and a global minimum a -2, but we will add noise to the function when the Gradient is calculated. We will then see how the momentum parameter affects convergence to a global minimum. \n"]},{"cell_type":"markdown","id":"853129d1-1ccc-4a89-8fa0-e9ba548e2dda","metadata":{},"outputs":[],"source":["with no momentum, we get stuck in a local minimum \n"]},{"cell_type":"code","id":"65b925e3-39ef-402e-931c-290565d5a0c0","metadata":{},"outputs":[],"source":["# Make the prediction without momentum when there is noise\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.001)\nplot_fourth_order(w, optimizer, std=10)"]},{"cell_type":"markdown","id":"c2ee54ef-e838-4656-8592-4a316c59c4d8","metadata":{},"outputs":[],"source":["with  momentum, we get to the global  minimum \n"]},{"cell_type":"code","id":"af5e6bd0-03f0-4c2e-8ece-8491f27fc8a3","metadata":{},"outputs":[],"source":["# Make the prediction with momentum when there is noise\n\noptimizer = torch.optim.SGD(w.parameters(), lr=0.001,momentum=0.9)\nplot_fourth_order(w, optimizer, std=10)"]},{"cell_type":"markdown","id":"daf7bc33-e287-4b52-82e2-d041bf718e77","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"ba720ae5-34b1-4917-96b9-99e5dd5d453b","metadata":{},"outputs":[],"source":["\u003ch3\u003ePractice\u003c/h3\u003e\n"]},{"cell_type":"markdown","id":"935ffff7-e50b-4ac1-ae1c-933c55846e93","metadata":{},"outputs":[],"source":["Create two \u003ccode\u003e SGD\u003c/code\u003e  objects with a learning rate of \u003ccode\u003e 0.001\u003c/code\u003e. Use the default momentum parameter value  for one and a value of \u003ccode\u003e 0.9\u003c/code\u003e for the second. Use the function \u003ccode\u003eplot_fourth_order\u003c/code\u003e with an \u003ccode\u003estd=100\u003c/code\u003e, to plot the different steps of each. Make sure you run the function on two independent cells.\n"]},{"cell_type":"code","id":"9852fc32-1199-4e55-b3e8-8dfe8a098f4f","metadata":{},"outputs":[],"source":["# Practice: Create two SGD optimizer with lr = 0.001, and one without momentum and the other with momentum = 0.9. Plot the result out.\n\n# Type your code here"]},{"cell_type":"markdown","id":"887502f1-f554-4e88-b12c-c3ac0ab026e0","metadata":{},"outputs":[],"source":["Double-click \u003cb\u003ehere\u003c/b\u003e for the solution.\n","\n","\u003c!-- \n","optimizer1 = torch.optim.SGD(w.parameters(), lr = 0.001)\n","plot_fourth_order(w, optimizer1, std = 100, color = 'black', paramlabel = 'parameter values with optimizer 1')\n","\n","optimizer2 = torch.optim.SGD(w.parameters(), lr = 0.001, momentum = 0.9)\n","plot_fourth_order(w, optimizer2, std = 100, color = 'red', paramlabel = 'parameter values with optimizer 2', objfun = False)\n"," --\u003e\n"]},{"cell_type":"markdown","id":"2fccd52e-d05e-4064-b8a8-4b23a67d4dfa","metadata":{},"outputs":[],"source":["\n","\u003ca href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2023-01-01\u0026context=cpdaas\u0026apps=data_science_experience%2Cwatson_machine_learning\"\u003e\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"\u003e\u003c/a\u003e\n"]},{"cell_type":"markdown","id":"9c6fc2ae-8f36-4c8a-9b26-6386cb1ed8f5","metadata":{},"outputs":[],"source":["\u003ch2\u003eAbout the Authors:\u003c/h2\u003e \n","\n","\u003ca href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2023-01-01\"\u003eJoseph Santarcangelo\u003c/a\u003e has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"79a11806-1b1e-4aa0-a91a-b81df8a84d5c","metadata":{},"outputs":[],"source":["Other contributors: \u003ca href=\"https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2023-01-01\"\u003eMichelle Carey\u003c/a\u003e, \u003ca href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\"\u003eMavis Zhou\u003c/a\u003e \n"]},{"cell_type":"markdown","id":"244cd80f-d092-485a-b208-07b6cede86e0","metadata":{},"outputs":[],"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","\n","\n","\u003chr\u003e\n","\n","## \u003ch3 align=\"center\"\u003e Â© IBM Corporation 2020. All rights reserved. \u003ch3/\u003e\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}