{"cells":[{"cell_type":"markdown","id":"a90f4fb2-f7ba-438b-8d72-60227631356f","metadata":{},"outputs":[],"source":["\u003cp style=\"text-align:center\"\u003e\n","    \u003ca href=\"https://skills.network/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\" target=\"_blank\"\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  /\u003e\n","    \u003c/a\u003e\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"ec87b49b-03c5-4a55-be33-f2ce5cad5105","metadata":{},"outputs":[],"source":["\u003ch1\u003eImage Datasets and Transforms\u003c/h1\u003e \n"]},{"cell_type":"markdown","id":"b0284fdf-4228-47d7-b63a-7b136d1077fe","metadata":{},"outputs":[],"source":["\u003ch2\u003eObjective\u003c/h2\u003e\u003cul\u003e\u003cli\u003e How to build a image dataset object.\u003c/li\u003e\u003cli\u003e How to perform pre-build transforms from Torchvision Transforms to the dataset. .\u003c/li\u003e\u003c/ul\u003e \n"]},{"cell_type":"markdown","id":"6983e7d7-d0f2-4a6e-ad61-292b5665d47b","metadata":{},"outputs":[],"source":["\u003ch2\u003eTable of Contents\u003c/h2\u003e\n","\u003cp\u003eIn this lab, you will build a dataset objects for images; many of the processes can be applied to a larger dataset. Then you will apply pre-build transforms from Torchvision Transforms to that dataset.\u003c/p\u003e\n","\u003cul\u003e\n","    \u003cli\u003e\u003ca href=\"#auxiliary\"\u003e Auxiliary Functions \u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Dataset\"\u003e Datasets\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Torchvision\"\u003eTorchvision Transforms\u003c/a\u003e\u003c/li\u003e\n","\u003c/ul\u003e\n","\u003cp\u003eEstimated Time Needed: \u003cstrong\u003e25 min\u003c/strong\u003e\u003c/p\u003e\n","\n","\u003chr\u003e\n"]},{"cell_type":"markdown","id":"3d3916e2-a14a-4fda-b2a8-93d93601b582","metadata":{},"outputs":[],"source":["\u003ch2\u003ePreparation\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"7ebb2484-94bd-49b8-8c5e-4a654fe9dfb1","metadata":{},"outputs":[],"source":["Download the dataset and unzip the files in your data directory, **to download faster this dataset has only 100 samples**:\n"]},{"cell_type":"code","id":"c9cdef7d-5970-43cb-9657-433d16b2287f","metadata":{},"outputs":[],"source":["! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/labs/Week1/data/img.tar.gz -P /resources/data\n"]},{"cell_type":"code","id":"39701637-7db0-43ba-8ff7-8e1a6e8cae63","metadata":{},"outputs":[],"source":["!tar -xf /resources/data/img.tar.gz "]},{"cell_type":"code","id":"d64cacf8-99e0-4ead-a0f1-ed537a84dcae","metadata":{},"outputs":[],"source":["!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/labs/Week1/data/index.csv "]},{"cell_type":"markdown","id":"9f3f60ec-5272-433c-9363-141647b9abf6","metadata":{},"outputs":[],"source":["We will use this function in the lab:\n"]},{"cell_type":"code","id":"0f09a576-46f7-4a41-b22c-5812ab146152","metadata":{},"outputs":[],"source":["def show_data(data_sample, shape = (28, 28)):\n    plt.imshow(data_sample[0].numpy().reshape(shape), cmap='gray')\n    plt.title('y = ' + data_sample[1])"]},{"cell_type":"markdown","id":"056f92f0-9ad3-4a25-b7e9-131780eefbcc","metadata":{},"outputs":[],"source":["The following are the libraries we are going to use for this lab. The \u003ccode\u003etorch.manual_seed()\u003c/code\u003e is for forcing the random function to give the same number every time we try to recompile it.\n"]},{"cell_type":"code","id":"14c1850d-f488-4781-993c-6dd33e4d436d","metadata":{},"outputs":[],"source":["# These are the libraries will be used for this lab.\n\nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\ntorch.manual_seed(0)"]},{"cell_type":"code","id":"34e1f412-4c7f-44a6-bd69-3095ff7cabaf","metadata":{},"outputs":[],"source":["from matplotlib.pyplot import imshow\nimport matplotlib.pylab as plt\nfrom PIL import Image\nimport pandas as pd\nimport os"]},{"cell_type":"markdown","id":"848b06e2-bdeb-44d7-b196-7e3d371a4a73","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"dc5c70a7-8656-41f7-9973-aab67c57c69a","metadata":{},"outputs":[],"source":["\u003ch2 id=\"auxiliary\"\u003eAuxiliary Functions\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"6ff34aa5-c13c-418a-95fb-3a512b1abc59","metadata":{},"outputs":[],"source":["You will use the following function as components of a dataset object, in this section, you will review each of the components independently.\n"]},{"cell_type":"markdown","id":"51a3fb61-bcbf-4a1c-88d2-f22ebe27e1d3","metadata":{},"outputs":[],"source":[" The path to the csv file with the labels for each image.\n"]},{"cell_type":"code","id":"0a15d7df-b979-4d27-8ce8-d320729a1ce7","metadata":{},"outputs":[],"source":["# Read CSV file from the URL and print out the first five samples\ndirectory=\"\"\ncsv_file ='index.csv'\ncsv_path=os.path.join(directory,csv_file)"]},{"cell_type":"markdown","id":"398d3c97-0310-47a8-9a65-6f5e900b6dc8","metadata":{},"outputs":[],"source":["You can load the CSV file and convert it into a dataframe , using the Pandas function \u003ccode\u003eread_csv()\u003c/code\u003e . You can view the dataframe using the method head.\n"]},{"cell_type":"code","id":"e06b76aa-3b51-4e42-9228-9730d756ecdf","metadata":{},"outputs":[],"source":["data_name = pd.read_csv(csv_path)\ndata_name.head()"]},{"cell_type":"markdown","id":"4265606b-fe5b-4ac2-a775-fd7e5af16384","metadata":{},"outputs":[],"source":["The first column of the dataframe corresponds to the type of clothing. The second column is the name of the image file corresponding to the clothing. You can obtain the path of the first file by using the method  \u003ccode\u003e \u003ci\u003eDATAFRAME\u003c/i\u003e.iloc[0, 1]\u003c/code\u003e. The first argument corresponds to the sample number, and the second input corresponds to the column index. \n"]},{"cell_type":"code","id":"858f0999-7949-47d5-a498-f6a6b955c017","metadata":{},"outputs":[],"source":["# Get the value on location row 0, column 1 (Notice that index starts at 0)\n#rember this dataset has only 100 samples to make the download faster  \nprint('File name:', data_name.iloc[0, 1])"]},{"cell_type":"markdown","id":"241b0d62-61e2-42cf-8550-4f697f220a04","metadata":{},"outputs":[],"source":["As the class of the sample is in the first column, you can also obtain the class value as follows.\n"]},{"cell_type":"code","id":"74463649-bc1c-4dd6-b3ba-70186152e028","metadata":{},"outputs":[],"source":["# Get the value on location row 0, column 0 (Notice that index starts at 0.)\n\nprint('y:', data_name.iloc[0, 0])"]},{"cell_type":"markdown","id":"2c1d4ea4-491f-4c48-822f-58df67fa8e80","metadata":{},"outputs":[],"source":["Similarly, You can obtain the file name of the second image file and class type:\n"]},{"cell_type":"code","id":"b65ca4b3-229d-4f91-b3cc-0d569a911f00","metadata":{},"outputs":[],"source":["# Print out the file name and the class number of the element on row 1 (the second row)\n\nprint('File name:', data_name.iloc[1, 1])\nprint('class or y:', data_name.iloc[1, 0])"]},{"cell_type":"markdown","id":"52d9c402-e464-41b9-bde6-7c8e6897ae1d","metadata":{},"outputs":[],"source":["The number of samples corresponds to the number of rows in a dataframe. You can obtain the number of rows using the following lines of code. This will correspond the data attribute \u003ccode\u003elen\u003c/code\u003e.\n"]},{"cell_type":"code","id":"c5d92d33-3dfe-4fd4-9c52-88c06f81610a","metadata":{},"outputs":[],"source":["# Print out the total number of rows in traing dataset\n\nprint('The number of rows: ', data_name.shape[0])"]},{"cell_type":"markdown","id":"71d475c4-aa33-4a77-8d84-c94cfde79390","metadata":{},"outputs":[],"source":["\u003ch2 id=\"load_image\"\u003eLoad Image\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"194029ec-e7b6-41e8-b772-0601803c2311","metadata":{},"outputs":[],"source":["To load the image, you need the directory and the image name. You can concatenate the variable \u003ccode\u003etrain_data_dir\u003c/code\u003e with the name of the image stored in a Dataframe. Finally, you will store the result in the variable \u003ccode\u003eimage_name\u003c/code\u003e\n"]},{"cell_type":"code","id":"317b8f64-fafe-42c3-8171-15bbf29079a4","metadata":{},"outputs":[],"source":["# Combine the directory path with file name\n\nimage_name =data_name.iloc[1, 1]\nimage_name"]},{"cell_type":"markdown","id":"f3b58a5a-580e-4d12-ae2b-e7a1930be853","metadata":{},"outputs":[],"source":["we can find the image path:\n"]},{"cell_type":"code","id":"6df27ab6-6320-4831-a6c4-70228811b63b","metadata":{},"outputs":[],"source":["image_path=os.path.join(directory,image_name)\nimage_path"]},{"cell_type":"markdown","id":"00511090-c533-45c1-8b0c-fe5de0511496","metadata":{},"outputs":[],"source":["You can then use the function \u003ccode\u003eImage.open\u003c/code\u003e to store the image to the variable \u003ccode\u003eimage\u003c/code\u003e and display the image and class .\n"]},{"cell_type":"code","id":"536ef8d4-1d7d-4adb-aeac-efefa874f7d1","metadata":{},"outputs":[],"source":["# Plot the second training image\n\nimage = Image.open(image_path)\nplt.imshow(image,cmap='gray', vmin=0, vmax=255)\nplt.title(data_name.iloc[1, 0])\nplt.show()"]},{"cell_type":"markdown","id":"0ddb6e2a-de4a-473e-ab7c-1a0c32c18b8f","metadata":{},"outputs":[],"source":["You can repeat the process for the 20th image.\n"]},{"cell_type":"code","id":"4034e6cd-685a-4a05-b12a-ba0b994f3e20","metadata":{},"outputs":[],"source":["# Plot the 20th image\n\nimage_name = data_name.iloc[19, 1]\nimage_path=os.path.join(directory,image_name)\nimage = Image.open(image_path)\nplt.imshow(image,cmap='gray', vmin=0, vmax=255)\nplt.title(data_name.iloc[19, 0])\nplt.show()"]},{"cell_type":"markdown","id":"241577fc-35c6-4634-affb-7f3d2655a920","metadata":{},"outputs":[],"source":["\u003chr\u003e\n"]},{"cell_type":"markdown","id":"41eb3382-7e7d-469e-a5c0-635bf478d47a","metadata":{},"outputs":[],"source":[" Create the dataset object.\n"]},{"cell_type":"markdown","id":"3844f284-4158-4c57-a5bd-7b84972efd92","metadata":{},"outputs":[],"source":["\u003ch2 id=\"data_class\"\u003eCreate a Dataset Class\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"8d1886d6-430d-4afe-9a8f-ac8e333b293a","metadata":{},"outputs":[],"source":["In this section, we will use the components in the last section to build a dataset class and then create an object.\n"]},{"cell_type":"code","id":"d8d52ea9-74d2-45db-a1b6-998518ff7644","metadata":{},"outputs":[],"source":["# Create your own dataset object\n\nclass Dataset(Dataset):\n\n    # Constructor\n    def __init__(self, csv_file, data_dir, transform=None):\n        \n        # Image directory\n        self.data_dir=data_dir\n        \n        # The transform is goint to be used on image\n        self.transform = transform\n        data_dircsv_file=os.path.join(self.data_dir,csv_file)\n        # Load the CSV file contians image info\n        self.data_name= pd.read_csv(data_dircsv_file)\n        \n        # Number of images in dataset\n        self.len=self.data_name.shape[0] \n    \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n        \n        # Image file path\n        img_name=os.path.join(self.data_dir,self.data_name.iloc[idx, 1])\n        # Open image file\n        image = Image.open(img_name)\n        \n        # The class label for the image\n        y = self.data_name.iloc[idx, 0]\n        \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y"]},{"cell_type":"code","id":"63f4eff1-b77d-4689-901a-e216c19bc31b","metadata":{},"outputs":[],"source":["# Create the dataset objects\n\ndataset = Dataset(csv_file=csv_file, data_dir=directory)"]},{"cell_type":"markdown","id":"e1ad0b65-969e-44a0-b1e4-65db519be193","metadata":{},"outputs":[],"source":["Each sample of the image and the class y is stored in a tuple \u003ccode\u003e dataset[sample]\u003c/code\u003e . The image is the first element in the tuple \u003ccode\u003e dataset[sample][0]\u003c/code\u003e the label or class is the second element in the tuple \u003ccode\u003e dataset[sample][1]\u003c/code\u003e. For example you can plot the first image and class.\n"]},{"cell_type":"code","id":"7b484725-36d2-47d8-9ce5-1558cb7d0df1","metadata":{},"outputs":[],"source":["image=dataset[0][0]\ny=dataset[0][1]\n\nplt.imshow(image,cmap='gray', vmin=0, vmax=255)\nplt.title(y)\nplt.show()"]},{"cell_type":"code","id":"9ccfc3d6-65a8-4c6e-a2ba-3f70d58afc9d","metadata":{},"outputs":[],"source":["y"]},{"cell_type":"markdown","id":"56f75305-a43e-4ceb-8cb6-bb4e0e8837d8","metadata":{},"outputs":[],"source":["Similarly, you can plot the second image: \n"]},{"cell_type":"code","id":"d988436f-2461-4a3b-8f67-8158d489239a","metadata":{},"outputs":[],"source":["image=dataset[9][0]\ny=dataset[9][1]\n\nplt.imshow(image,cmap='gray', vmin=0, vmax=255)\nplt.title(y)\nplt.show()"]},{"cell_type":"markdown","id":"10a37fac-a555-4ecc-b96d-c9c209c0469d","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Torchvision\"\u003e Torchvision Transforms  \u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"b8946517-c03a-4aa9-bf7a-1e036a25426a","metadata":{},"outputs":[],"source":[" \n","You will focus on the following libraries:\n"]},{"cell_type":"code","id":"1e0ed42a-0816-4a63-9030-d97f00e452ef","metadata":{},"outputs":[],"source":["import torchvision.transforms as transforms"]},{"cell_type":"markdown","id":"40ff647a-3fc1-41ec-8d9b-d7d8628b3f63","metadata":{},"outputs":[],"source":["We can apply some image transform functions on the dataset object. The iamge can be cropped and converted to a tensor. We can use \u003ccode\u003etransform.Compose\u003c/code\u003e we learned from the previous lab to combine the two transform functions.\n"]},{"cell_type":"code","id":"f060bd79-4fc1-4dc4-80e5-3e982c2c4a26","metadata":{},"outputs":[],"source":["# Combine two transforms: crop and convert to tensor. Apply the compose to MNIST dataset\n\ncroptensor_data_transform = transforms.Compose([transforms.CenterCrop(20), transforms.ToTensor()])\ndataset = Dataset(csv_file=csv_file , data_dir=directory,transform=croptensor_data_transform )\nprint(\"The shape of the first element tensor: \", dataset[0][0].shape)\n"]},{"cell_type":"markdown","id":"0832fc4e-0fda-4b95-aa3f-1a1790ac670b","metadata":{},"outputs":[],"source":["We can see the image is now 20 x 20\n"]},{"cell_type":"markdown","id":"0dbaf83f-86f9-4e51-892d-852cec062d7a","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"9c072f0b-1b40-4409-96a5-8b2708b6fc9b","metadata":{},"outputs":[],"source":["Let us plot the first image again. Notice we see less of the shoe.\n"]},{"cell_type":"code","id":"d6ed41e4-531e-4a9c-a776-4931b4b6668a","metadata":{},"outputs":[],"source":["# Plot the first element in the dataset\n\nshow_data(dataset[0],shape = (20, 20))"]},{"cell_type":"code","id":"cde23923-4c92-4839-92a9-d70521c19e5c","metadata":{},"outputs":[],"source":["# Plot the second element in the dataset\n\nshow_data(dataset[1],shape = (20, 20))"]},{"cell_type":"markdown","id":"a73c6a6b-6528-454d-b839-2be0ad61a593","metadata":{},"outputs":[],"source":["In the below example, we Vertically flip the image, and then convert it to a tensor. Use \u003ccode\u003etransforms.Compose()\u003c/code\u003e to combine these two transform functions. Plot the flipped image.\n"]},{"cell_type":"code","id":"d7808fca-7961-4fe7-8f35-61c4ed10ce00","metadata":{},"outputs":[],"source":["# Construct the compose. Apply it on MNIST dataset. Plot the image out.\n\nfliptensor_data_transform = transforms.Compose([transforms.RandomVerticalFlip(p=1),transforms.ToTensor()])\ndataset = Dataset(csv_file=csv_file , data_dir=directory,transform=fliptensor_data_transform )\nshow_data(dataset[1])"]},{"cell_type":"markdown","id":"cfac198a-c28e-4efd-bcfe-df6074f0f160","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"e93b65cf-13e5-4bbc-8289-464d7e64ed65","metadata":{},"outputs":[],"source":["\u003ch3\u003ePractice\u003c/h3\u003e\n"]},{"cell_type":"markdown","id":"84f09e54-ae3d-44b2-9235-99fe0b34e5d0","metadata":{},"outputs":[],"source":["Try to use the \u003ccode\u003eRandomVerticalFlip\u003c/code\u003e (vertically flip the image) with horizontally flip and convert to tensor as a compose. Apply the compose on image. Use \u003ccode\u003eshow_data()\u003c/code\u003e to plot the second image (the image as \u003cb\u003e2\u003c/b\u003e).\n"]},{"cell_type":"code","id":"77b24521-78a2-4c9f-a3c9-b2027f104d11","metadata":{},"outputs":[],"source":["# Practice: Combine vertical flip, horizontal flip and convert to tensor as a compose. Apply the compose on image. Then plot the image\n\n# Type your code here"]},{"cell_type":"markdown","id":"fd983b21-71b0-4864-8609-21cf4302d855","metadata":{},"outputs":[],"source":["Double-click __here__ for the solution.\n","\u003c!-- \n","my_data_transform = transforms.Compose([transforms.RandomVerticalFlip(p = 1), transforms.RandomHorizontalFlip(p = 1), transforms.ToTensor()])\n","dataset = Dataset(csv_file=csv_file , data_dir=directory,transform=fliptensor_data_transform )\n","show_data(dataset[1])\n"," --\u003e\n"]},{"cell_type":"markdown","id":"e01c95a4-b31d-4c87-a4c1-451c09c9f839","metadata":{},"outputs":[],"source":["\u003ca href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\u0026context=cpdaas\u0026apps=data_science_experience%2Cwatson_machine_learning\"\u003e\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"\u003e\u003c/a\u003e\n"]},{"cell_type":"markdown","id":"0345dab2-8c4b-464a-a502-793a5cacbc99","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"c8814522-7444-47fc-9ae7-7ba8e4a2db7b","metadata":{},"outputs":[],"source":["\u003ch2\u003eAbout the Authors:\u003c/h2\u003e \n","\n","\u003ca href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\"\u003eJoseph Santarcangelo\u003c/a\u003e has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"c79d2360-bd8a-4c3b-98c1-f3f179f0225f","metadata":{},"outputs":[],"source":["Other contributors: \u003ca href=\"https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\"\u003eMichelle Carey\u003c/a\u003e, \u003ca href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\"\u003eMavis Zhou\u003c/a\u003e \n"]},{"cell_type":"markdown","id":"e5582142-7573-40b0-aa0b-4f2cae2e5bc1","metadata":{},"outputs":[],"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-21  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n"]},{"cell_type":"markdown","id":"38013452-160f-46fb-a22f-9416f79cbf4b","metadata":{},"outputs":[],"source":["\u003chr\u003e\n"]},{"cell_type":"markdown","id":"70d79763-9735-401a-830c-0e62e3cc8096","metadata":{},"outputs":[],"source":["## \u003ch3 align=\"center\"\u003e Â© IBM Corporation 2020. All rights reserved. \u003ch3/\u003e\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}