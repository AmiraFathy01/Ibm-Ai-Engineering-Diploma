{"cells":[{"cell_type":"markdown","id":"372c7ed2-27ed-461c-8384-05a15b526774","metadata":{},"outputs":[],"source":["\u003cp style=\"text-align:center\"\u003e\n","    \u003ca href=\"https://skills.network/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\" target=\"_blank\"\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  /\u003e\n","    \u003c/a\u003e\n","\u003c/p\u003e\n","\u003ch1 align=center\u003e\u003cfont size = 5\u003eConvolutional Neral Network Simple example \u003c/font\u003e\u003c/h1\u003e \n"]},{"cell_type":"markdown","id":"5a244940-2c10-46bd-8e4e-ad8ae01e927b","metadata":{},"outputs":[],"source":["\n","\u003ch3\u003eObjective for this Notebook\u003ch3\u003e    \n","\u003ch5\u003e 1. Learn Convolutional Neral Network\u003c/h5\u003e\n","\u003ch5\u003e 2. Define Softmax , Criterion function, Optimizer and Train the  Model\u003c/h5\u003e    \n","\n"]},{"cell_type":"markdown","id":"1b0ceeea-ae1b-4ceb-8a5e-2f61b7670367","metadata":{},"outputs":[],"source":["\n","# Table of Contents\n","In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines \n","\n","\u003cdiv class=\"alert alert-block alert-info\" style=\"margin-top: 20px\"\u003e\n","\u003cli\u003e\u003ca href=\"#ref0\"\u003eHelper functions \u003c/a\u003e\u003c/li\u003e\n","\n","\u003cli\u003e\u003ca href=\"#ref1\"\u003e Prepare Data \u003c/a\u003e\u003c/li\u003e\n","\u003cli\u003e\u003ca href=\"#ref2\"\u003eConvolutional Neral Network \u003c/a\u003e\u003c/li\u003e\n","\u003cli\u003e\u003ca href=\"#ref3\"\u003eDefine Softmax , Criterion function, Optimizer and Train the  Model\u003c/a\u003e\u003c/li\u003e\n","\u003cli\u003e\u003ca href=\"#ref4\"\u003eAnalyse Results\u003c/a\u003e\u003c/li\u003e\n","\n","\u003cbr\u003e\n","\u003cp\u003e\u003c/p\u003e\n","Estimated Time Needed: \u003cstrong\u003e25 min\u003c/strong\u003e\n","\u003c/div\u003e\n","\n","\u003chr\u003e\n"]},{"cell_type":"markdown","id":"3b4a86a4-8b43-45e9-bd2e-b005219fe4f4","metadata":{},"outputs":[],"source":["\u003ca id=\"ref0\"\u003e\u003c/a\u003e\n","\u003ch2 align=center\u003eHelper functions \u003c/h2\u003e\n"]},{"cell_type":"code","id":"49a133b2-9571-41f5-8f8e-2b8673f9b2a3","metadata":{},"outputs":[],"source":["import torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd"]},{"cell_type":"code","id":"c5bd7abc-4afd-4a6d-bd7c-ac1cc592f45d","metadata":{},"outputs":[],"source":["torch.manual_seed(4)"]},{"cell_type":"markdown","id":"29316db1-fc4e-4bfa-a77b-96b07caa4acc","metadata":{},"outputs":[],"source":["function to plot out the parameters of the Convolutional layers  \n"]},{"cell_type":"code","id":"0bf76c4e-c54c-4f88-b181-21dc1f2792d9","metadata":{},"outputs":[],"source":["def plot_channels(W):\n    #number of output channels \n    n_out=W.shape[0]\n    #number of input channels \n    n_in=W.shape[1]\n    w_min=W.min().item()\n    w_max=W.max().item()\n    fig, axes = plt.subplots(n_out,n_in)\n    fig.subplots_adjust(hspace = 0.1)\n    out_index=0\n    in_index=0\n    #plot outputs as rows inputs as columns \n    for ax in axes.flat:\n    \n        if in_index\u003en_in-1:\n            out_index=out_index+1\n            in_index=0\n              \n        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        in_index=in_index+1\n\n    plt.show()"]},{"cell_type":"markdown","id":"1a7d9b23-3062-4b6e-bc57-8a20766a816d","metadata":{},"outputs":[],"source":["\u003ccode\u003eshow_data\u003c/code\u003e: plot out data sample\n"]},{"cell_type":"code","id":"88e4dace-79f8-4c18-ad3c-622215e4ef1b","metadata":{},"outputs":[],"source":["def show_data(dataset,sample):\n\n    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n    plt.title('y='+str(dataset.y[sample].item()))\n    plt.show()"]},{"cell_type":"markdown","id":"202ab209-3482-482e-a445-e34ba7172206","metadata":{},"outputs":[],"source":["create some toy data \n"]},{"cell_type":"code","id":"52841ea5-18de-4bd2-b5fb-bcd76f6ca2f4","metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\nclass Data(Dataset):\n    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n        \"\"\"\n        p:portability that pixel is wight  \n        N_images:number of images \n        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n        \"\"\"\n        if train==True:\n            np.random.seed(1)  \n        \n        #make images multiple of 3 \n        N_images=2*(N_images//2)\n        images=np.zeros((N_images,1,11,11))\n        start1=3\n        start2=1\n        self.y=torch.zeros(N_images).type(torch.long)\n\n        for n in range(N_images):\n            if offset\u003e0:\n        \n                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n            else:\n                low=4\n                high=1\n        \n            if n\u003c=N_images//2:\n                self.y[n]=0\n                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n            elif  n\u003eN_images//2:\n                self.y[n]=1\n                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n           \n        \n        \n        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n        self.len=self.x.shape[0]\n        del(images)\n        np.random.seed(0)\n    def __getitem__(self,index):      \n        return self.x[index],self.y[index]\n    def __len__(self):\n        return self.len"]},{"cell_type":"markdown","id":"6e25e502-bcbb-486b-a356-751ea87a474e","metadata":{},"outputs":[],"source":["\u003ccode\u003eplot_activation\u003c/code\u003e: plot out the activations of the Convolutional layers  \n"]},{"cell_type":"code","id":"36ea2424-4b5c-454b-86ee-f52265bd2272","metadata":{},"outputs":[],"source":["def plot_activations(A,number_rows= 1,name=\"\"):\n    A=A[0,:,:,:].detach().numpy()\n    n_activations=A.shape[0]\n    \n    \n    print(n_activations)\n    A_min=A.min().item()\n    A_max=A.max().item()\n\n    if n_activations==1:\n\n        # Plot the image.\n        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n\n    else:\n        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n        fig.subplots_adjust(hspace = 0.4)\n        for i,ax in enumerate(axes.flat):\n            if i\u003c n_activations:\n                # Set the label for the sub-plot.\n                ax.set_xlabel( \"activation:{0}\".format(i+1))\n\n                # Plot the image.\n                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n                ax.set_xticks([])\n                ax.set_yticks([])\n    plt.show()"]},{"cell_type":"markdown","id":"b3e47bff-2cb3-4b3f-ace8-791da6cec54b","metadata":{},"outputs":[],"source":["\n","Utility function for computing output of convolutions\n","takes a tuple of (h,w) and returns a tuple of (h,w)\n"]},{"cell_type":"code","id":"8b9487f6-8b04-4bd1-9407-8ecc9d9c81cf","metadata":{},"outputs":[],"source":["\ndef conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    #by Duane Nielsen\n    from math import floor\n    if type(kernel_size) is not tuple:\n        kernel_size = (kernel_size, kernel_size)\n    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n    return h, w"]},{"cell_type":"markdown","id":"caef31fe-225b-4a06-b925-d1da6e0618de","metadata":{},"outputs":[],"source":["\u003ca id=\"ref1\"\u003e\u003c/a\u003e\n","\u003ch2 align=center\u003ePrepare Data \u003c/h2\u003e \n"]},{"cell_type":"markdown","id":"ca99bfc6-9ab0-45cf-9676-14e6be84670e","metadata":{},"outputs":[],"source":["Load the training dataset with 10000 samples \n"]},{"cell_type":"code","id":"bf878100-3aa8-4851-9253-a4550344966c","metadata":{},"outputs":[],"source":["N_images=10000\ntrain_dataset=Data(N_images=N_images)"]},{"cell_type":"markdown","id":"066c9ac0-3776-40c2-a915-058761c343b5","metadata":{},"outputs":[],"source":["Load the testing dataset\n"]},{"cell_type":"code","id":"2804e04a-f4c6-4b33-a2ef-02831adcb5fc","metadata":{},"outputs":[],"source":["validation_dataset=Data(N_images=1000,train=False)\nvalidation_dataset"]},{"cell_type":"markdown","id":"e4f76952-8ee3-431c-a46b-8d91023ce603","metadata":{},"outputs":[],"source":["we can see the data type is long \n"]},{"cell_type":"markdown","id":"8e12dae0-ae35-41c8-8cfe-a7f68cf0b448","metadata":{},"outputs":[],"source":["### Data Visualization \n"]},{"cell_type":"markdown","id":"9483540f-938b-436c-8c9e-008ef815f6af","metadata":{},"outputs":[],"source":["Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"]},{"cell_type":"markdown","id":"c7270411-02f2-4d0b-99de-e39f8b4ace0f","metadata":{},"outputs":[],"source":["We can print out the third label \n"]},{"cell_type":"code","id":"bebaac1c-641d-4a3b-89dd-b85f93af008b","metadata":{},"outputs":[],"source":["show_data(train_dataset,0)"]},{"cell_type":"code","id":"2c26564d-fa40-4946-a6c4-7000fb3863b8","metadata":{},"outputs":[],"source":["show_data(train_dataset,N_images//2+2)"]},{"cell_type":"markdown","id":"1d84c987-f3f8-482d-a365-725bb185af4d","metadata":{},"outputs":[],"source":["we can plot the 3rd  sample \n"]},{"cell_type":"markdown","id":"a3f33ada-7ea2-48c7-91e1-99a83293251a","metadata":{},"outputs":[],"source":["\u003ca id=\"ref3\"\u003e\u003c/a\u003e\n","### Build a Convolutional Neral Network Class \n"]},{"cell_type":"markdown","id":"d48caaab-0877-485e-8ae8-defd3b69ed7c","metadata":{},"outputs":[],"source":["The input image is 11 x11, the following will change the size of the activations:\n","\u003cul\u003e\n","\u003cil\u003econvolutional layer\u003c/il\u003e \n","\u003c/ul\u003e\n","\u003cul\u003e\n","\u003cil\u003emax pooling layer\u003c/il\u003e \n","\u003c/ul\u003e\n","\u003cul\u003e\n","\u003cil\u003econvolutional layer \u003c/il\u003e\n","\u003c/ul\u003e\n","\u003cul\u003e\n","\u003cil\u003emax pooling layer \u003c/il\u003e\n","\u003c/ul\u003e\n","\n","with the following parameters \u003ccode\u003ekernel_size\u003c/code\u003e, \u003ccode\u003estride\u003c/code\u003e and \u003ccode\u003e pad\u003c/code\u003e.\n","We use the following  lines of code to change the image before we get tot he fully connected layer \n"]},{"cell_type":"code","id":"db7f2d52-75c6-45bc-9a66-b9e947227970","metadata":{},"outputs":[],"source":["out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out)\nout1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out1)\nout2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out2)\n\nout3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out3)"]},{"cell_type":"markdown","id":"ee89cbec-7617-4307-9e58-b0f50fa99318","metadata":{},"outputs":[],"source":["Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"]},{"cell_type":"code","id":"46e9692d-45a6-4022-95d0-2245602ef30a","metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n    def __init__(self,out_1=2,out_2=1):\n        \n        super(CNN,self).__init__()\n        #first Convolutional layers \n        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n\n        #second Convolutional layers\n        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n        #max pooling \n\n        #fully connected layer \n        self.fc1=nn.Linear(out_2*7*7,2)\n        \n    def forward(self,x):\n        #first Convolutional layers\n        x=self.cnn1(x)\n        #activation function \n        x=torch.relu(x)\n        #max pooling \n        x=self.maxpool1(x)\n        #first Convolutional layers\n        x=self.cnn2(x)\n        #activation function\n        x=torch.relu(x)\n        #max pooling\n        x=self.maxpool2(x)\n        #flatten output \n        x=x.view(x.size(0),-1)\n        #fully connected layer\n        x=self.fc1(x)\n        return x\n    \n    def activations(self,x):\n        #outputs activation this is not necessary just for fun \n        z1=self.cnn1(x)\n        a1=torch.relu(z1)\n        out=self.maxpool1(a1)\n        \n        z2=self.cnn2(out)\n        a2=torch.relu(z2)\n        out=self.maxpool2(a2)\n        out=out.view(out.size(0),-1)\n        return z1,a1,z2,a2,out        "]},{"cell_type":"markdown","id":"3157611e-8685-4ebb-8b85-3df57bb4ad45","metadata":{},"outputs":[],"source":["\u003ca id=\"ref3\"\u003e\u003c/a\u003e\n","\u003ch2\u003e Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  \u003c/h2\u003e \n"]},{"cell_type":"markdown","id":"faa04725-942b-4494-b114-3e7798aa0436","metadata":{},"outputs":[],"source":["There are 2 output channels for the first layer, and 1 outputs channel for the second layer \n"]},{"cell_type":"code","id":"bb80d1c0-2026-481e-92ed-b0933dfb5269","metadata":{},"outputs":[],"source":["model=CNN(2,1)"]},{"cell_type":"markdown","id":"31f0d39c-44af-4e5f-9003-1c45d6a543d6","metadata":{},"outputs":[],"source":["we can see the model parameters with the object \n"]},{"cell_type":"code","id":"01d47bf0-555a-4e71-a0b7-02ffe3f64e33","metadata":{},"outputs":[],"source":["model"]},{"cell_type":"markdown","id":"763b2652-060d-4198-8fab-4e65d2bd838f","metadata":{},"outputs":[],"source":["Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"]},{"cell_type":"code","id":"f8809f0b-53a0-4517-8138-2b04288ecbee","metadata":{},"outputs":[],"source":["\nplot_channels(model.state_dict()['cnn1.weight'])\n"]},{"cell_type":"markdown","id":"839e7e20-1870-4084-a603-d8634ec2fc7b","metadata":{},"outputs":[],"source":["Loss function \n"]},{"cell_type":"code","id":"868d5c3e-8e0d-47de-9157-258aa2459c17","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","id":"1ee105ab-b987-4d7d-b4ec-46b4300b7b4e","metadata":{},"outputs":[],"source":["Define the loss function \n"]},{"cell_type":"code","id":"2d74dbb1-4c05-4cbe-9489-1b7fb35945b9","metadata":{},"outputs":[],"source":["criterion=nn.CrossEntropyLoss()"]},{"cell_type":"markdown","id":"174ee8ad-4f00-4812-87cf-4cd85661d5b9","metadata":{},"outputs":[],"source":[" optimizer class \n"]},{"cell_type":"code","id":"8b348d67-ccd5-479a-a29e-7783d161fb3d","metadata":{},"outputs":[],"source":["learning_rate=0.001\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","id":"40f49b04-0b01-4d24-bcb7-3b8a47ea3eca","metadata":{},"outputs":[],"source":["Define the optimizer class \n"]},{"cell_type":"code","id":"0356cfb7-41cd-4629-aed2-34b3816f8cd9","metadata":{},"outputs":[],"source":["\ntrain_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\nvalidation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"]},{"cell_type":"markdown","id":"e756e750-b6d6-4ebe-87cf-9e7f6b07a24c","metadata":{},"outputs":[],"source":["Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"]},{"cell_type":"code","id":"f1c3d898-aa49-47c7-8576-e757a34f0149","metadata":{},"outputs":[],"source":["n_epochs=10\ncost_list=[]\naccuracy_list=[]\nN_test=len(validation_dataset)\ncost=0\n#n_epochs\nfor epoch in range(n_epochs):\n    cost=0    \n    for x, y in train_loader:\n      \n\n        #clear gradient \n        optimizer.zero_grad()\n        #make a prediction \n        z=model(x)\n        # calculate loss \n        loss=criterion(z,y)\n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        cost+=loss.item()\n    cost_list.append(cost)\n        \n        \n    correct=0\n    #perform a prediction on the validation  data  \n    for x_test, y_test in validation_loader:\n\n        z=model(x_test)\n        _,yhat=torch.max(z.data,1)\n\n        correct+=(yhat==y_test).sum().item()\n        \n\n    accuracy=correct/N_test\n\n    accuracy_list.append(accuracy)\n    \n\n"]},{"cell_type":"markdown","id":"60b249cc-9b1c-477f-96b1-dc25f7b380fc","metadata":{},"outputs":[],"source":["#### \u003ca id=\"ref3\"\u003e\u003c/a\u003e\n","\u003ch2 align=center\u003eAnalyse Results\u003c/h2\u003e \n"]},{"cell_type":"markdown","id":"303aa89f-1725-40a7-bced-bfc05528b0e2","metadata":{},"outputs":[],"source":["Plot the loss and accuracy on the validation data:\n"]},{"cell_type":"code","id":"600ed645-a5dc-443b-bdd0-4788f8f7f037","metadata":{},"outputs":[],"source":["fig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list,color=color)\nax1.set_xlabel('epoch',color=color)\nax1.set_ylabel('total loss',color=color)\nax1.tick_params(axis='y', color=color)\n    \nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color)  \nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', labelcolor=color)\nfig.tight_layout()"]},{"cell_type":"markdown","id":"bc6f1872-bf69-4a56-901e-e4da108e43bf","metadata":{},"outputs":[],"source":["View the results of the parameters for the Convolutional layers \n"]},{"cell_type":"code","id":"9548384d-7efb-46a3-8071-e5f44b696bba","metadata":{},"outputs":[],"source":["model.state_dict()['cnn1.weight']"]},{"cell_type":"code","id":"26d876c0-c16f-40af-9425-bc2a3cd3f719","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn1.weight'])"]},{"cell_type":"code","id":"6f544a9c-1379-45bc-98fc-1cb0f5606a75","metadata":{},"outputs":[],"source":["model.state_dict()['cnn1.weight']"]},{"cell_type":"code","id":"ed1eaa45-7a6b-42bb-8e0f-857cdcb51760","metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","id":"5218e910-8a83-46cc-b1d7-128293ea7944","metadata":{},"outputs":[],"source":["Consider the following sample \n"]},{"cell_type":"code","id":"3d92ec62-836a-4404-9c47-45dcc13eb056","metadata":{},"outputs":[],"source":["show_data(train_dataset,N_images//2+2)"]},{"cell_type":"markdown","id":"4ee579c7-4934-44d1-a9d7-49520ab1e717","metadata":{},"outputs":[],"source":["Determine the activations \n"]},{"cell_type":"code","id":"35bf969e-a800-4203-a1ca-a536b0dce045","metadata":{},"outputs":[],"source":["out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\nout=model.activations(train_dataset[0][0].view(1,1,11,11))"]},{"cell_type":"markdown","id":"56473000-48b1-4da6-8ccd-11f6739a78de","metadata":{},"outputs":[],"source":["Plot them out\n"]},{"cell_type":"code","id":"057cf579-5e10-4f2e-a179-72c3da5b20e6","metadata":{},"outputs":[],"source":["plot_activations(out[0],number_rows=1,name=\" feature map\")\nplt.show()\n"]},{"cell_type":"code","id":"5b8af467-ad56-43c6-be55-179745ee05fa","metadata":{},"outputs":[],"source":["plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\nplt.show()"]},{"cell_type":"code","id":"c13c42f6-0273-476d-bd84-3683f9d52f15","metadata":{},"outputs":[],"source":["plot_activations(out[3],number_rows=1,name=\"first feature map\")\nplt.show()"]},{"cell_type":"markdown","id":"f082baa1-8f90-44fa-a9d3-575857205d7c","metadata":{},"outputs":[],"source":["we save the output of the activation after flattening  \n"]},{"cell_type":"code","id":"c3afbab2-b8e8-4322-8727-a08450f3b053","metadata":{},"outputs":[],"source":["out1=out[4][0].detach().numpy()"]},{"cell_type":"markdown","id":"8f448ca7-dac9-40a2-8212-d8930deef11e","metadata":{},"outputs":[],"source":["we can do the same for a sample  where y=0 \n"]},{"cell_type":"code","id":"c2634e55-8017-4020-80fa-8c22f947200d","metadata":{},"outputs":[],"source":["out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\nout0"]},{"cell_type":"code","id":"5a938dea-ad72-427d-8011-d7b86bc1875f","metadata":{},"outputs":[],"source":["plt.subplot(2, 1, 1)\nplt.plot( out1, 'b')\nplt.title('Flatted Activation Values  ')\nplt.ylabel('Activation')\nplt.xlabel('index')\nplt.subplot(2, 1, 2)\nplt.plot(out0, 'r')\nplt.xlabel('index')\nplt.ylabel('Activation')"]},{"cell_type":"markdown","id":"f3d9b2ad-c021-40e4-b25d-531ab85faeca","metadata":{},"outputs":[],"source":["\n","\n","\u003ca href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\u0026context=cpdaas\u0026apps=data_science_experience%2Cwatson_machine_learning\"\u003e\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"\u003e\u003c/a\u003e\n","\n"]},{"cell_type":"markdown","id":"845f0daf-88e4-40ee-91da-6b343fa868eb","metadata":{},"outputs":[],"source":["### About the Authors:  \n","[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n","\n","Other contributors: [Michelle Carey](https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01) \n"]},{"cell_type":"markdown","id":"137ab383-1d52-49ce-b1b6-50adb23493ff","metadata":{},"outputs":[],"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","\n","\n","\u003chr\u003e\n","\n","## \u003ch3 align=\"center\"\u003e Â© IBM Corporation 2020. All rights reserved. \u003ch3/\u003e\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}