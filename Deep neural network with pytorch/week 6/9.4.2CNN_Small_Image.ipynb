{"cells":[{"cell_type":"markdown","id":"2f7835b0-aadf-4e27-aec1-0462a8e85282","metadata":{},"outputs":[],"source":["\u003cp style=\"text-align:center\"\u003e\n","    \u003ca href=\"https://skills.network/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\" target=\"_blank\"\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  /\u003e\n","    \u003c/a\u003e\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"07c731a5-5d94-426d-a83a-26b3ba2a06b1","metadata":{},"outputs":[],"source":["\u003ch1\u003eConvolutional Neural Network with Small Images\u003c/h1\u003e \n"]},{"cell_type":"markdown","id":"bf5149c4-cfaf-4004-8d92-e202813a2253","metadata":{},"outputs":[],"source":["\n","\u003ch3\u003eObjective for this Notebook\u003ch3\u003e    \n","\u003ch5\u003e 1. Learn how to use a Convolutional Neural Network to classify handwritten digits from the MNIST database\u003c/h5\u003e\n","\u003ch5\u003e 2. Learn hot to reshape the images to make them faster to process \u003c/h5\u003e     \n","\n"]},{"cell_type":"markdown","id":"0cd986af-d196-4ded-963b-6bcac129ab3b","metadata":{},"outputs":[],"source":["\u003ch2\u003eTable of Contents\u003c/h2\u003e\n","\u003cp\u003eIn this lab, we will use a Convolutional Neural Network to classify handwritten digits from the MNIST database. We will reshape the images to make them faster to process \u003c/p\u003e\n","\n","\u003cul\u003e\n","\u003cli\u003e\u003ca href=\"#Makeup_Data\"\u003eGet Some Data\u003c/a\u003e\u003c/li\u003e\n","\u003cli\u003e\u003ca href=\"#CNN\"\u003eConvolutional Neural Network\u003c/a\u003e\u003c/li\u003e\n","\u003cli\u003e\u003ca href=\"#Train\"\u003eDefine Softmax, Criterion function, Optimizer and Train the Model\u003c/a\u003e\u003c/li\u003e\n","\u003cli\u003e\u003ca href=\"#Result\"\u003eAnalyze Results\u003c/a\u003e\u003c/li\u003e\n","\u003c/ul\u003e\n","\u003cp\u003eEstimated Time Needed: \u003cstrong\u003e25 min\u003c/strong\u003e 14 min to train model \u003c/p\u003e\n","\n","\u003chr\u003e\n"]},{"cell_type":"markdown","id":"a03ca2a0-dfb1-4db3-94a4-e4c281361419","metadata":{},"outputs":[],"source":["\u003ch2\u003ePreparation\u003c/h2\u003e\n"]},{"cell_type":"code","id":"cbff056d-c540-4e06-9bd6-f0ea62160904","metadata":{},"outputs":[],"source":["\n\n# Import the libraries we need to use in this lab\n\n# Using the following line code to install the torchvision library\n# !mamba install -y torchvision\n\n!pip install torchvision==0.9.1 torch==1.8.1 \nimport torch \nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\nimport matplotlib.pylab as plt\nimport numpy as np"]},{"cell_type":"markdown","id":"8b04a526-3214-4e6b-9ee2-baf238372449","metadata":{},"outputs":[],"source":["Define the function \u003ccode\u003eplot_channels\u003c/code\u003e to plot out the kernel parameters of  each channel \n"]},{"cell_type":"code","id":"b73bf799-c4ec-470e-ae17-6fd470b305c5","metadata":{},"outputs":[],"source":["# Define the function for plotting the channels\n\ndef plot_channels(W):\n    n_out = W.shape[0]\n    n_in = W.shape[1]\n    w_min = W.min().item()\n    w_max = W.max().item()\n    fig, axes = plt.subplots(n_out, n_in)\n    fig.subplots_adjust(hspace=0.1)\n    out_index = 0\n    in_index = 0\n    \n    #plot outputs as rows inputs as columns \n    for ax in axes.flat:\n        if in_index \u003e n_in-1:\n            out_index = out_index + 1\n            in_index = 0\n        ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        in_index = in_index + 1\n\n    plt.show()"]},{"cell_type":"markdown","id":"23950c9f-4144-4b20-a7bc-dd7411f6d289","metadata":{},"outputs":[],"source":["Define the function \u003ccode\u003eplot_parameters\u003c/code\u003e to plot out the kernel parameters of each channel with Multiple outputs . \n"]},{"cell_type":"code","id":"468ffc94-fd1c-435c-8c2a-882d580bf330","metadata":{},"outputs":[],"source":["# Define the function for plotting the parameters\n\ndef plot_parameters(W, number_rows=1, name=\"\", i=0):\n    W = W.data[:, i, :, :]\n    n_filters = W.shape[0]\n    w_min = W.min().item()\n    w_max = W.max().item()\n    fig, axes = plt.subplots(number_rows, n_filters // number_rows)\n    fig.subplots_adjust(hspace=0.4)\n\n    for i, ax in enumerate(axes.flat):\n        if i \u003c n_filters:\n            # Set the label for the sub-plot.\n            ax.set_xlabel(\"kernel:{0}\".format(i + 1))\n\n            # Plot the image.\n            ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')\n            ax.set_xticks([])\n            ax.set_yticks([])\n    plt.suptitle(name, fontsize=10)    \n    plt.show()"]},{"cell_type":"markdown","id":"afe4ef8c-d318-408c-a7b5-50391c3bd2ad","metadata":{},"outputs":[],"source":["Define the function \u003ccode\u003eplot_activation\u003c/code\u003e to plot out the activations of the Convolutional layers  \n"]},{"cell_type":"code","id":"f718c601-48a8-4fe8-a293-eb7c203a9da4","metadata":{},"outputs":[],"source":["# Define the function for plotting the activations\n\ndef plot_activations(A, number_rows=1, name=\"\", i=0):\n    A = A[0, :, :, :].detach().numpy()\n    n_activations = A.shape[0]\n    A_min = A.min().item()\n    A_max = A.max().item()\n    fig, axes = plt.subplots(number_rows, n_activations // number_rows)\n    fig.subplots_adjust(hspace = 0.4)\n\n    for i, ax in enumerate(axes.flat):\n        if i \u003c n_activations:\n            # Set the label for the sub-plot.\n            ax.set_xlabel(\"activation:{0}\".format(i + 1))\n\n            # Plot the image.\n            ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')\n            ax.set_xticks([])\n            ax.set_yticks([])\n    plt.show()"]},{"cell_type":"markdown","id":"15179816-5df4-4933-8848-14a053a3fc85","metadata":{},"outputs":[],"source":["Define the function \u003ccode\u003eshow_data\u003c/code\u003e to plot out data samples as images.\n"]},{"cell_type":"code","id":"90119e9a-e4d9-4121-8533-503953f0b61c","metadata":{},"outputs":[],"source":["def show_data(data_sample):\n    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n    plt.title('y = '+ str(data_sample[1]))"]},{"cell_type":"markdown","id":"1346f03c-48fc-4e99-bd53-191157ec5698","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"16f5769f-5ee8-495c-8aec-28e4d06ae479","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Makeup_Data\"\u003eGet the Data\u003c/h2\u003e \n"]},{"cell_type":"markdown","id":"7dddf4d5-18d7-4a38-a88b-b7fbe56dc184","metadata":{},"outputs":[],"source":["we create a transform to resize the image and convert it to a tensor .\n"]},{"cell_type":"code","id":"437565c1-f5b0-44f4-bdad-fe7b5fe123dd","metadata":{},"outputs":[],"source":["\n\nIMAGE_SIZE = 16\n\n\ncomposed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])"]},{"cell_type":"markdown","id":"40cf4a17-f78f-4a4c-9044-203c72de3542","metadata":{},"outputs":[],"source":["Load the training dataset by setting the parameters \u003ccode\u003etrain \u003c/code\u003e to \u003ccode\u003eTrue\u003c/code\u003e. We use the transform defined above.\n"]},{"cell_type":"code","id":"5757e293-c5af-4b13-9482-231a76c18c5b","metadata":{},"outputs":[],"source":["\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)"]},{"cell_type":"markdown","id":"13d0f2fe-061d-4b1b-a972-9a602e21c917","metadata":{},"outputs":[],"source":["Load the testing dataset by setting the parameters train  \u003ccode\u003eFalse\u003c/code\u003e.\n"]},{"cell_type":"code","id":"fbf8bcd8-b897-4041-8ace-6cd0b714377c","metadata":{},"outputs":[],"source":["# Make the validating \n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)"]},{"cell_type":"markdown","id":"82cde4b0-619b-4131-92c9-ee5041a56f21","metadata":{},"outputs":[],"source":["We can see the data type is long.\n"]},{"cell_type":"code","id":"488a9ebd-a21d-4f6b-b77e-1c6f7b889dc3","metadata":{},"outputs":[],"source":["# Show the data type for each element in dataset\n\ntype(train_dataset[0][1])"]},{"cell_type":"markdown","id":"cdbefb5d-a516-4c0c-81e7-8cdc319c5019","metadata":{},"outputs":[],"source":["Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image.\n"]},{"cell_type":"markdown","id":"cf889041-d2ee-4aca-b9e8-bae6e3d7d4f2","metadata":{},"outputs":[],"source":["\u003cimg src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.2.1imagenet.png\" width=\"550\" alt=\"MNIST data image\"\u003e\n"]},{"cell_type":"markdown","id":"2788b96c-b63e-4d5a-93f2-f91df25b07ad","metadata":{},"outputs":[],"source":["Print out the fourth label \n"]},{"cell_type":"code","id":"3c89f56c-69d4-47c5-be98-551432d34258","metadata":{},"outputs":[],"source":["# The label for the fourth data element\n\ntrain_dataset[3][1]"]},{"cell_type":"markdown","id":"7d3f047b-28ca-4e27-8c30-8c7fbd92576c","metadata":{},"outputs":[],"source":["Plot the fourth sample \n"]},{"cell_type":"code","id":"fabc2c8d-efc9-4d3e-8458-791afb785c35","metadata":{},"outputs":[],"source":["# The image for the fourth data element\nshow_data(train_dataset[3])\n"]},{"cell_type":"markdown","id":"35a8bcf0-5d67-406d-9834-9d986e37c67f","metadata":{},"outputs":[],"source":["The fourth sample is a \"1\".\n"]},{"cell_type":"markdown","id":"8bf86ea8-7a2b-462a-a619-d4958ffd9f16","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"e2e4561f-c832-407e-8826-16ca17db7d20","metadata":{},"outputs":[],"source":["\u003ch2 id=\"CNN\"\u003eBuild a Convolutional Neural Network Class\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"33aac107-93d9-45c7-89fe-b3ea92d20009","metadata":{},"outputs":[],"source":["Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"]},{"cell_type":"code","id":"aaa17f30-6de5-4d49-929a-accc5a0eda6e","metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n    \n    # Contructor\n    def __init__(self, out_1=16, out_2=32):\n        super(CNN, self).__init__()\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n\n        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n    \n    # Prediction\n    def forward(self, x):\n        x = self.cnn1(x)\n        x = torch.relu(x)\n        x = self.maxpool1(x)\n        x = self.cnn2(x)\n        x = torch.relu(x)\n        x = self.maxpool2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        return x\n    \n    # Outputs in each steps\n    def activations(self, x):\n        #outputs activation this is not necessary\n        z1 = self.cnn1(x)\n        a1 = torch.relu(z1)\n        out = self.maxpool1(a1)\n        \n        z2 = self.cnn2(out)\n        a2 = torch.relu(z2)\n        out1 = self.maxpool2(a2)\n        out = out.view(out.size(0),-1)\n        return z1, a1, z2, a2, out1,out"]},{"cell_type":"markdown","id":"19bfc499-ebb7-4ce1-b305-a76933b82c16","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Train\"\u003eDefine the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the Model\u003c/h2\u003e \n"]},{"cell_type":"markdown","id":"0ef0856f-cf15-4abf-8b8d-cfdb2a59b1a6","metadata":{},"outputs":[],"source":["There are 16 output channels for the first layer, and 32 output channels for the second layer \n"]},{"cell_type":"code","id":"6cdf9e0f-a20c-4aed-b86f-3c2655b054da","metadata":{},"outputs":[],"source":["# Create the model object using CNN class\n\nmodel = CNN(out_1=16, out_2=32)"]},{"cell_type":"markdown","id":"37810040-628f-4ffc-b049-b7bd7ebd3555","metadata":{},"outputs":[],"source":["Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"]},{"cell_type":"code","id":"6c39912d-e2c9-4884-90e6-a93cba9ee0c3","metadata":{},"outputs":[],"source":["# Plot the parameters\n\nplot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name=\"1st layer kernels before training \")\nplot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' )"]},{"cell_type":"markdown","id":"2dab126f-d9a5-49b2-ad9f-a19093be8248","metadata":{},"outputs":[],"source":["Define the loss function, the optimizer and the dataset loader \n"]},{"cell_type":"code","id":"9ffd5d7b-0857-4519-b0fb-23d1aa67d3fc","metadata":{},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"]},{"cell_type":"markdown","id":"f1a7442d-5487-49aa-9712-6bef24eb4c71","metadata":{},"outputs":[],"source":["Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"]},{"cell_type":"code","id":"6078600d-97cc-4ad8-8a1d-d53d82f5b8bb","metadata":{},"outputs":[],"source":["# Train the model\n\nn_epochs=3\ncost_list=[]\naccuracy_list=[]\nN_test=len(validation_dataset)\nCOST=0\n\ndef train_model(n_epochs):\n    for epoch in range(n_epochs):\n        COST=0\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            z = model(x)\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            COST+=loss.data\n        \n        cost_list.append(COST)\n        correct=0\n        #perform a prediction on the validation  data  \n        for x_test, y_test in validation_loader:\n            z = model(x_test)\n            _, yhat = torch.max(z.data, 1)\n            correct += (yhat == y_test).sum().item()\n        accuracy = correct / N_test\n        accuracy_list.append(accuracy)\n     \ntrain_model(n_epochs)"]},{"cell_type":"markdown","id":"13d7a796-671f-4476-b7cd-8e5152bb74ec","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"3353f066-08a3-4a35-aa10-ecc78e1ff933","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Result\"\u003eAnalyze Results\u003c/h2\u003e \n"]},{"cell_type":"markdown","id":"b00a4147-fd9e-4988-9b5a-c5a48199b156","metadata":{},"outputs":[],"source":["Plot the loss and accuracy on the validation data:\n"]},{"cell_type":"code","id":"be076ea4-b3e1-4434-a95b-9e94996b19bc","metadata":{},"outputs":[],"source":["# Plot the loss and accuracy\n\nfig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list, color=color)\nax1.set_xlabel('epoch', color=color)\nax1.set_ylabel('Cost', color=color)\nax1.tick_params(axis='y', color=color)\n    \nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color) \nax2.set_xlabel('epoch', color=color)\nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', color=color)\nfig.tight_layout()"]},{"cell_type":"markdown","id":"f7bcfbee-7486-4a7e-8359-0c13241a6760","metadata":{},"outputs":[],"source":["View the results of the parameters for the Convolutional layers \n"]},{"cell_type":"code","id":"04f2e641-b0ae-479b-b7f1-b0d37a749c83","metadata":{},"outputs":[],"source":["# Plot the channels\n\nplot_channels(model.state_dict()['cnn1.weight'])\nplot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","id":"a4c5cd81-c4c3-4195-9dd8-2107bd8fb406","metadata":{},"outputs":[],"source":["Consider the following sample \n"]},{"cell_type":"code","id":"70f19aeb-e90e-42bc-9604-3964b2540369","metadata":{},"outputs":[],"source":["# Show the second image\n\nshow_data(train_dataset[1])"]},{"cell_type":"markdown","id":"844d308a-7664-4205-9ec7-1468de681286","metadata":{},"outputs":[],"source":["Determine the activations \n"]},{"cell_type":"code","id":"363715a2-1219-4564-b846-d6ca67d03cd7","metadata":{},"outputs":[],"source":["# Use the CNN activations class to see the steps\n\nout = model.activations(train_dataset[1][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE))"]},{"cell_type":"markdown","id":"a19c0112-ef9b-4244-97a6-4f74abe8b978","metadata":{},"outputs":[],"source":["Plot out the first set of activations \n"]},{"cell_type":"code","id":"9ab69f1a-d44f-4694-a98f-cf1929ad961e","metadata":{},"outputs":[],"source":["# Plot the outputs after the first CNN\n\nplot_activations(out[0], number_rows=4, name=\"Output after the 1st CNN\")"]},{"cell_type":"markdown","id":"e074dcd2-195e-44bb-8c41-618eab8fbe26","metadata":{},"outputs":[],"source":["The image below is the result after applying the relu activation function \n"]},{"cell_type":"code","id":"38a66e67-e942-4f50-a2b9-425c3110f2e7","metadata":{},"outputs":[],"source":["# Plot the outputs after the first Relu\n\nplot_activations(out[1], number_rows=4, name=\"Output after the 1st Relu\")"]},{"cell_type":"markdown","id":"8b76ce57-5f12-4c92-ace6-20aeedcc05ef","metadata":{},"outputs":[],"source":["The image below is the result of the activation map after the second output layer.\n"]},{"cell_type":"code","id":"f6ea20cb-6c7f-4020-845f-98e436593aa5","metadata":{},"outputs":[],"source":["# Plot the outputs after the second CNN\n\nplot_activations(out[2], number_rows=32 // 4, name=\"Output after the 2nd CNN\")"]},{"cell_type":"markdown","id":"ddf4f10a-29b4-490d-9cd8-5f5750868c12","metadata":{},"outputs":[],"source":["The image below is the result of the activation map after applying the second relu  \n"]},{"cell_type":"code","id":"2b3af31d-9b95-40c7-a4ae-f5b14ab08b50","metadata":{},"outputs":[],"source":["# Plot the outputs after the second Relu\n\nplot_activations(out[3], number_rows=4, name=\"Output after the 2nd Relu\")"]},{"cell_type":"markdown","id":"91650def-f8d8-46bf-9676-9ef1a8aa41b0","metadata":{},"outputs":[],"source":["We can  see the result for the third sample \n"]},{"cell_type":"code","id":"76b7b75a-1fd1-4d3c-8734-b791271ebbee","metadata":{},"outputs":[],"source":["# Show the third image\n\nshow_data(train_dataset[2])"]},{"cell_type":"code","id":"9787e924-0ea1-4a3c-aec8-b3113c496190","metadata":{},"outputs":[],"source":["# Use the CNN activations class to see the steps\n\nout = model.activations(train_dataset[2][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE))"]},{"cell_type":"code","id":"78129184-b69f-438d-b840-f9d43663ff1e","metadata":{},"outputs":[],"source":["# Plot the outputs after the first CNN\n\nplot_activations(out[0], number_rows=4, name=\"Output after the 1st CNN\")"]},{"cell_type":"code","id":"cdbb2690-396e-4fca-a158-fd9e3992fe25","metadata":{},"outputs":[],"source":["# Plot the outputs after the first Relu\n\nplot_activations(out[1], number_rows=4, name=\"Output after the 1st Relu\")"]},{"cell_type":"code","id":"627ae7c7-7b9f-41dd-acaa-b542dd8ea385","metadata":{},"outputs":[],"source":["# Plot the outputs after the second CNN\n\nplot_activations(out[2], number_rows=32 // 4, name=\"Output after the 2nd CNN\")"]},{"cell_type":"code","id":"332593d9-24a9-4fa6-8970-10bbeb9f1e97","metadata":{},"outputs":[],"source":["# Plot the outputs after the second Relu\n\nplot_activations(out[3], number_rows=4, name=\"Output after the 2nd Relu\")"]},{"cell_type":"markdown","id":"dd5656d1-e8cb-4eee-b9e3-3ef2424133f2","metadata":{},"outputs":[],"source":["Plot the first five mis-classified samples:\n"]},{"cell_type":"code","id":"b09c5716-6544-44dc-987a-f2139b286033","metadata":{},"outputs":[],"source":["# Plot the mis-classified samples\n\ncount = 0\nfor x, y in torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1):\n    z = model(x)\n    _, yhat = torch.max(z, 1)\n    if yhat != y:\n        show_data((x, y))\n        plt.show()\n        print(\"yhat: \",yhat)\n        count += 1\n    if count \u003e= 5:\n        break  "]},{"cell_type":"markdown","id":"648eff37-8a68-4b87-b402-01d3a3f4cff2","metadata":{},"outputs":[],"source":["\n","\u003ca href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\u0026context=cpdaas\u0026apps=data_science_experience%2Cwatson_machine_learning\"\u003e\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"\u003e\u003c/a\u003e\n"]},{"cell_type":"markdown","id":"099c0eda-92f9-446c-b9e9-ebf48f88ba8f","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"b17d0c55-6466-45a9-bfbc-5a54666672cf","metadata":{},"outputs":[],"source":["\u003ch2\u003eAbout the Authors:\u003c/h2\u003e \n","\n","\u003ca href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\"\u003eJoseph Santarcangelo\u003c/a\u003e has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"b17f6c63-3d47-487a-b272-fd0b6a40c794","metadata":{},"outputs":[],"source":["Other contributors: \u003ca href=\"https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\"\u003eMichelle Carey\u003c/a\u003e, \u003ca href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\"\u003eMavis Zhou\u003c/a\u003e\n"]},{"cell_type":"markdown","id":"c758ba66-d70a-4d64-9de3-afd4ea63b02e","metadata":{},"outputs":[],"source":["Thanks to Magnus \u003ca href=\"http://www.hvass-labs.org/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\"\u003eErik Hvass Pedersen\u003c/a\u003e whose tutorials helped me understand convolutional Neural Network\n"]},{"cell_type":"markdown","id":"a4397041-d01b-491d-84ad-d0f9dcdd20ab","metadata":{},"outputs":[],"source":["\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","\n","\n","\u003chr\u003e\n","\n","## \u003ch3 align=\"center\"\u003e © IBM Corporation 2020. All rights reserved. \u003ch3/\u003e\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}